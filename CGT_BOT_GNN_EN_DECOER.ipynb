{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJrNFbbrAdAl",
        "outputId": "6c660d18-88c0-427a-b6fa-32529b8dfe8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "The code is using device:cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import sent_tokenize,word_tokenize\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#downloading nltk data\n",
        "try:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt_tab')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "#setting device\n",
        "if torch.cuda.is_available():\n",
        "    dev='cuda'\n",
        "else:\n",
        "    dev='cpu'\n",
        "\n",
        "device=torch.device(device=dev)\n",
        "print(f\"The code is using device:{device}\")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and pretraining\n",
        "with open(file=r\"/content/drive/MyDrive/wiki.train.tokens\", mode=\"r\", encoding='utf-8') as f:\n",
        "    wiki_text = f.read()\n",
        "    sentences = sent_tokenize(text=wiki_text)[:40000]\n",
        "    pre_training_text = [' '.join(sentences[i:i+4]) for i in range(0, len(sentences), 4)]\n",
        "\n",
        "print(f\"Loaded {len(pre_training_text)} pre-training samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UMYX9JxBGGw",
        "outputId": "db0ed695-8eaf-4324-e859-fcf3a6055692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10000 pre-training samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBV-nlaNrPC2",
        "outputId": "de77e6a0-8fce-44de-cb20-3cc2738a87bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.57.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.bos_token = tokenizer.eos_token\n",
        "print(\"Tokenizer initiated\")\n",
        "\n",
        "embedding_model = SentenceTransformer(model_name_or_path='sentence-transformers/all-MiniLM-L6-v2')\n",
        "print(\"Embedding model initiated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lGG2f_tB_Lu",
        "outputId": "5368386c-90e6-493f-821b-bfe75f6803de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer initiated\n",
            "Embedding model initiated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Config\n",
        "\n",
        "GPT2Config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOoOtgo3nCB2",
        "outputId": "f158019b-85b0-4e21-a439-396a78d44785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 50256,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_embd\": 768,\n",
              "  \"n_head\": 12,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 12,\n",
              "  \"n_positions\": 1024,\n",
              "  \"reorder_and_upcast_attn\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_by_inverse_layer_idx\": false,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"transformers_version\": \"4.57.0\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50257\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-2 pretrained embedding source\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "nqhRnUE7dGFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONBt84bgF_AK",
        "outputId": "1d672246-1ad7-4515-b8eb-fa86026c7182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "pdf_path = r\"/content/Fed_Meta_Allign.pdf\"\n",
        "reader = PdfReader(pdf_path)\n",
        "\n",
        "file_text = \"\"\n",
        "for page in reader.pages:\n",
        "    file_text += page.extract_text() + \"\\n\"\n"
      ],
      "metadata": {
        "id": "fJfVLuTHAqco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_text"
      ],
      "metadata": {
        "id": "4td3xBbvGf-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process file document for fine-tuning\n",
        "file_sentences = sent_tokenize(file_text)\n",
        "finetune_data = [sent.strip() for sent in file_sentences if len(sent) > 10]\n",
        "print(f\"Processed {len(finetune_data)} fine-tuning samples from file document\")\n",
        "\n",
        "# Create document chunks for RAG retrieval\n",
        "document_chunks = []\n",
        "paragraphs = file_text.split('\\n\\n')\n",
        "for para in paragraphs:\n",
        "    para = para.strip()\n",
        "    if len(para) > 40:\n",
        "        if len(para) > 200:\n",
        "            sentences = sent_tokenize(para)\n",
        "            for i in range(0, len(sentences), 2):\n",
        "                chunk = ' '.join(sentences[i:i+2])\n",
        "                if len(chunk) > 40:\n",
        "                    document_chunks.append(chunk)\n",
        "        else:\n",
        "            document_chunks.append(para)\n",
        "\n",
        "print(f\"Created {len(document_chunks)} retrieval chunks\")\n",
        "print(\"Data loading complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENSIwrrHGChP",
        "outputId": "feab881c-ec81-44aa-fa5c-5f32cd7456c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 276 fine-tuning samples from file document\n",
            "Created 142 retrieval chunks\n",
            "Data loading complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def load_qa_pairs_optimized(file_path):\n",
        "    \"\"\"Optimized Q&A loader for your specific format\"\"\"\n",
        "    qa_pairs = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Robust regex pattern that handles:\n",
        "    # - Q1/Q2/Q3 repeating patterns\n",
        "    # - Multi-line answers\n",
        "    # - Various spacing formats\n",
        "    pattern = re.compile(r'Q\\d+:\\s*(.*?)\\s*A\\d+:\\s*(.*?)(?=\\s*Q\\d+:|$)', re.DOTALL)\n",
        "\n",
        "    matches = pattern.findall(content)\n",
        "\n",
        "    for question, answer in matches:\n",
        "        # Clean and normalize\n",
        "        question = ' '.join(question.strip().split())\n",
        "        answer = ' '.join(answer.strip().split())\n",
        "\n",
        "        # Quality checks\n",
        "        if (question and answer and\n",
        "            len(question) > 10 and  # Reasonable question length\n",
        "            len(answer) > 20 and    # Reasonable answer length\n",
        "            not question.lower().startswith('based on') and  # Filter meta-questions\n",
        "            not answer.lower().startswith('error')):         # Filter errors\n",
        "\n",
        "            formatted_qa = f\"Question: {question}\\nAnswer: {answer}\"\n",
        "            qa_pairs.append(formatted_qa)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    seen = set()\n",
        "    unique_qa_pairs = []\n",
        "    for qa in qa_pairs:\n",
        "        if qa not in seen:\n",
        "            seen.add(qa)\n",
        "            unique_qa_pairs.append(qa)\n",
        "\n",
        "    return unique_qa_pairs\n",
        "\n",
        "# Load and verify Q&A data\n",
        "print(\"Loading Q&A pairs...\")\n",
        "qa_data = load_qa_pairs_optimized(\"filtered_qna.txt\")\n",
        "print(f\"Successfully loaded {len(qa_data)} high-quality Q&A pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7TA-WUaBcJp",
        "outputId": "5f2e8994-690c-43e9-ae36-51d812dfd620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Q&A pairs...\n",
            "Successfully loaded 272 high-quality Q&A pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show samples\n",
        "print(\"\\nSample Q&A pairs:\")\n",
        "for i, pair in enumerate(qa_data[:3]):\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQQ65PdvBu2X",
        "outputId": "17144e8e-6600-4e57-d81e-bdd16cb0af74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Q&A pairs:\n",
            "\n",
            "--- Sample 1 ---\n",
            "Question: What is the primary application area highlighted for Fed-Meta-Align?\n",
            "Answer: The primary application area is real-time fault classification in resource-constrained IoT devices, specifically focusing on industrial safety. This suggests a need for robust models deployed on edge devices.\n",
            "\n",
            "--- Sample 2 ---\n",
            "Question: What is a key limitation of standard Federated Learning (FL) that Fed-Meta-Align aims to address?\n",
            "Answer: Standard FL often struggles with non-IID (non-independent and identically distributed) data. This means data distributions vary significantly across different IoT devices, hindering model performance.\n",
            "\n",
            "--- Sample 3 ---\n",
            "Question: What is the primary goal of the Fed-Meta-Align framework as described in the text?\n",
            "Answer: Fed-Meta-Align aims to address model divergence issues encountered in federated learning scenarios. It does this by establishing a robust initialization and training pipeline for models deployed on IoT devices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrate with your SLM training\n",
        "print(f\"\\nIntegrating with SLM training...\")\n",
        "print(f\"Original fine-tuning samples: {len(finetune_data)}\")\n",
        "enhanced_finetune_data = finetune_data + qa_data\n",
        "print(f\"Enhanced fine-tuning samples: {len(enhanced_finetune_data)}\")\n",
        "print(f\"Q&A enrichment: +{len(qa_data)} samples ({len(qa_data)/len(enhanced_finetune_data)*100:.1f}% of total)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okCydX_ECDte",
        "outputId": "057f7ce7-f22d-4f3a-fa83-1df41efc13db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Integrating with SLM training...\n",
            "Original fine-tuning samples: 276\n",
            "Enhanced fine-tuning samples: 548\n",
            "Q&A enrichment: +272 samples (49.6% of total)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Config\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "!pip install torch_geometric\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "from torch_geometric.nn import GCNConv, GATv2Conv\n",
        "from torch_geometric.data import Data, Batch\n",
        "print(\"PyTorch Geometric installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-DoidFzGGjp",
        "outputId": "f4a177e6-2da2-488d-ad73-e664c0c82546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "PyTorch Geometric installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CGTConfig:\n",
        "    \"\"\"Configuration for GNN + Encoder + Decoder\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257\n",
        "        self.hidden_dim = 768\n",
        "        self.gnn_layers = 2\n",
        "        self.transformer_encoder_layers = 4\n",
        "        self.transformer_decoder_layers = 2\n",
        "        self.num_heads = 8\n",
        "        self.gnn_type = 'gat'\n",
        "        self.dropout = 0.1\n",
        "        self.max_seq_len = 512\n",
        "\n",
        "config = CGTConfig()\n",
        "\n",
        "class GNNEncoderDecoder(nn.Module):\n",
        "    \"\"\" GNN + Encoder + Decoder without fusion\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Input embeddings\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(config.max_seq_len, config.hidden_dim)\n",
        "\n",
        "        # GNN Layers for local context\n",
        "        self.gnn_layers = nn.ModuleList()\n",
        "        for i in range(config.gnn_layers):\n",
        "            if config.gnn_type == 'gat':\n",
        "                self.gnn_layers.append(GATv2Conv(config.hidden_dim, config.hidden_dim))\n",
        "            else:\n",
        "                self.gnn_layers.append(GCNConv(config.hidden_dim, config.hidden_dim))\n",
        "\n",
        "        self.gnn_ln = nn.LayerNorm(config.hidden_dim)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config.hidden_dim,\n",
        "            nhead=config.num_heads,\n",
        "            dim_feedforward=4 * config.hidden_dim,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.transformer_encoder_layers)\n",
        "\n",
        "        # Transformer Decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=config.hidden_dim,\n",
        "            nhead=config.num_heads,\n",
        "            dim_feedforward=4 * config.hidden_dim,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=config.transformer_decoder_layers)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(config.hidden_dim, config.vocab_size)\n",
        "\n",
        "        # Copy GPT-2 embeddings\n",
        "        self.embedding.weight.data = gpt2.transformer.wte.weight.data.clone()\n",
        "\n",
        "        # Tie output projection weights to embeddings\n",
        "        self.output_layer.weight = self.embedding.weight\n",
        "\n",
        "        print(\"✅ GPT-2 embeddings loaded into Corrected CGT model!\")\n",
        "\n",
        "\n",
        "        total_params = sum(p.numel() for p in self.parameters()) / 1e6\n",
        "        print(f\"Corrected Model: {config.gnn_layers} GNN + {config.transformer_encoder_layers} Encoder + {config.transformer_decoder_layers} Decoder\")\n",
        "        print(f\"Total parameters: {total_params:.1f}M\")\n",
        "\n",
        "    def create_graph(self, input_ids):\n",
        "        \"\"\"Create graph structure from input tokens\"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        graphs = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            # Create node features with embeddings + positional encoding\n",
        "            token_embeds = self.embedding(input_ids[b])\n",
        "            pos_embeds = self.pos_embedding(torch.arange(seq_len, device=input_ids.device))\n",
        "            x = token_embeds + pos_embeds\n",
        "\n",
        "            # Create edges (linear chain with skip connections)\n",
        "            edge_index = []\n",
        "            for i in range(seq_len - 1):\n",
        "                edge_index.append([i, i + 1])  # Forward\n",
        "                edge_index.append([i + 1, i])  # Backward\n",
        "\n",
        "            # Skip connections for better information flow\n",
        "            for i in range(seq_len - 2):\n",
        "                edge_index.append([i, i + 2])\n",
        "                edge_index.append([i + 2, i])\n",
        "\n",
        "            if edge_index:\n",
        "                edge_index = torch.tensor(edge_index, dtype=torch.long, device=input_ids.device).t().contiguous()\n",
        "            else:\n",
        "                edge_index = torch.empty((2, 0), device=input_ids.device, dtype=torch.long)\n",
        "\n",
        "            graphs.append(Data(x, edge_index))\n",
        "\n",
        "        return Batch.from_data_list(graphs)\n",
        "\n",
        "    def encode(self, input_ids, attention_mask=None):\n",
        "        \"\"\"Encode input through GNN + Encoder\"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # Process through GNN\n",
        "        graph_batch = self.create_graph(input_ids)\n",
        "        x_gnn = graph_batch.x\n",
        "\n",
        "        for gnn_layer in self.gnn_layers:\n",
        "            x_gnn = F.relu(gnn_layer(x_gnn, graph_batch.edge_index))\n",
        "            x_gnn = F.dropout(x_gnn, p=self.config.dropout, training=self.training)\n",
        "\n",
        "        x_gnn = self.gnn_ln(x_gnn)\n",
        "\n",
        "        # Reshape for transformer\n",
        "        x_reshaped = x_gnn.view(batch_size, seq_len, -1)\n",
        "\n",
        "        # Process through encoder\n",
        "        if attention_mask is not None:\n",
        "            src_key_padding_mask = ~attention_mask.bool()\n",
        "        else:\n",
        "            src_key_padding_mask = None\n",
        "\n",
        "        memory = self.encoder(x_reshaped, src_key_padding_mask=src_key_padding_mask)\n",
        "        return memory\n",
        "\n",
        "    def decode(self, tgt_ids, memory, memory_mask=None, tgt_mask=None):\n",
        "        \"\"\"Decode using transformer decoder\"\"\"\n",
        "        batch_size, tgt_len = tgt_ids.shape\n",
        "\n",
        "        # Prepare target embeddings\n",
        "        tgt_embeddings = self.embedding(tgt_ids)\n",
        "        tgt_positions = torch.arange(tgt_len, device=tgt_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
        "        tgt_embeddings = tgt_embeddings + self.pos_embedding(tgt_positions)\n",
        "\n",
        "        # Decode\n",
        "        decoder_output = self.decoder(\n",
        "            tgt=tgt_embeddings,\n",
        "            memory=memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_key_padding_mask=memory_mask\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        logits = self.output_layer(decoder_output)\n",
        "        return logits\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"Generate causal mask for decoder\"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, input_ids, target_ids=None, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for training - encoder-decoder with teacher forcing\n",
        "\n",
        "        \"\"\"\n",
        "        # Encode source\n",
        "        memory = self.encode(input_ids, attention_mask)\n",
        "\n",
        "        # Use provided target_ids or default to input_ids\n",
        "        if target_ids is None:\n",
        "            target_ids = input_ids\n",
        "\n",
        "        tgt_mask = self.generate_square_subsequent_mask(target_ids.size(1)).to(input_ids.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            memory_mask = ~attention_mask.bool()\n",
        "        else:\n",
        "            memory_mask = None\n",
        "\n",
        "        # Decode\n",
        "        logits = self.decode(target_ids, memory, memory_mask, tgt_mask)\n",
        "        return logits\n",
        "\n",
        "# Initialize the model\n",
        "model = GNNEncoderDecoder(config).to(device)\n",
        "print(f\"Corrected Model initialized with {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6Vjr4tHGLQB",
        "outputId": "bf68f1b3-126c-4a0c-d93d-549905bfcea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPT-2 embeddings loaded into Corrected CGT model!\n",
            "Corrected Model: 2 GNN + 4 Encoder + 2 Decoder\n",
            "Total parameters: 88.7M\n",
            "Corrected Model initialized with 88.7M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_language_model(model, data, tokenizer, epochs=None, lr=None, stage_name=\"Pretraining\",batch_size=None):\n",
        "    \"\"\"\n",
        "    General language model pretraining function for learning English patterns\n",
        "\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    print(f\"🚀 {stage_name} for {epochs} epochs with lr={lr}...\")\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        batch_size = batch_size\n",
        "\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch_texts = data[i:i+batch_size]\n",
        "\n",
        "            # Tokenize with proper special tokens for language modeling\n",
        "            inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=256,\n",
        "                return_tensors='pt',\n",
        "                add_special_tokens=True\n",
        "            )\n",
        "\n",
        "            input_ids = inputs['input_ids'].to(device)\n",
        "            attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "            # Create proper decoder input (shifted right)\n",
        "            # For encoder-decoder: decoder sees [tok0, tok1, ..., tokN-1]\n",
        "            # and predicts [tok1, tok2, ..., tokN]\n",
        "            decoder_input_ids = input_ids[:, :-1]  # Remove last token for decoder input\n",
        "            labels = input_ids[:, 1:]  # Remove first token for labels (shifted left)\n",
        "\n",
        "            # Forward pass with separate source and target\n",
        "            outputs = model(input_ids, target_ids=decoder_input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Calculate loss - outputs already aligned with labels\n",
        "            loss = loss_function(\n",
        "                outputs.contiguous().view(-1, outputs.size(-1)),\n",
        "                labels.contiguous().view(-1)\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            if batch_count % 10 == 0:\n",
        "                print(f\"{stage_name} Epoch {epoch+1}/{epochs}, Batch {batch_count}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
        "        print(f\"✅ {stage_name} Epoch {epoch+1} completed, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(f\"🎉 {stage_name} completed successfully!\")"
      ],
      "metadata": {
        "id": "mAHJnJt1GTdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🚀 Starting Pretraining on English...\")\n",
        "pretrain_language_model(model, pre_training_text, tokenizer, epochs=5, lr=5e-5, stage_name=\"English Pretraining\",batch_size=16)"
      ],
      "metadata": {
        "id": "ehPYdunPDRqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ee376a-c007-4a96-f320-cd6538b8cbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Pretraining on English...\n",
            "🚀 English Pretraining for 5 epochs with lr=5e-05...\n",
            "English Pretraining Epoch 1/5, Batch 10, Loss: 8.9555\n",
            "English Pretraining Epoch 1/5, Batch 20, Loss: 8.5515\n",
            "English Pretraining Epoch 1/5, Batch 30, Loss: 8.9300\n",
            "English Pretraining Epoch 1/5, Batch 40, Loss: 8.4266\n",
            "English Pretraining Epoch 1/5, Batch 50, Loss: 8.2238\n",
            "English Pretraining Epoch 1/5, Batch 60, Loss: 7.7157\n",
            "English Pretraining Epoch 1/5, Batch 70, Loss: 8.2317\n",
            "English Pretraining Epoch 1/5, Batch 80, Loss: 8.0523\n",
            "English Pretraining Epoch 1/5, Batch 90, Loss: 8.1297\n",
            "English Pretraining Epoch 1/5, Batch 100, Loss: 8.2169\n",
            "English Pretraining Epoch 1/5, Batch 110, Loss: 8.3675\n",
            "English Pretraining Epoch 1/5, Batch 120, Loss: 7.3589\n",
            "English Pretraining Epoch 1/5, Batch 130, Loss: 7.8536\n",
            "English Pretraining Epoch 1/5, Batch 140, Loss: 7.7360\n",
            "English Pretraining Epoch 1/5, Batch 150, Loss: 7.7464\n",
            "English Pretraining Epoch 1/5, Batch 160, Loss: 7.6600\n",
            "English Pretraining Epoch 1/5, Batch 170, Loss: 8.1327\n",
            "English Pretraining Epoch 1/5, Batch 180, Loss: 7.7924\n",
            "English Pretraining Epoch 1/5, Batch 190, Loss: 7.8536\n",
            "English Pretraining Epoch 1/5, Batch 200, Loss: 7.5516\n",
            "English Pretraining Epoch 1/5, Batch 210, Loss: 7.6142\n",
            "English Pretraining Epoch 1/5, Batch 220, Loss: 7.9174\n",
            "English Pretraining Epoch 1/5, Batch 230, Loss: 7.6219\n",
            "English Pretraining Epoch 1/5, Batch 240, Loss: 7.6399\n",
            "English Pretraining Epoch 1/5, Batch 250, Loss: 7.6424\n",
            "English Pretraining Epoch 1/5, Batch 260, Loss: 7.5712\n",
            "English Pretraining Epoch 1/5, Batch 270, Loss: 7.3900\n",
            "English Pretraining Epoch 1/5, Batch 280, Loss: 7.4278\n",
            "English Pretraining Epoch 1/5, Batch 290, Loss: 7.2947\n",
            "English Pretraining Epoch 1/5, Batch 300, Loss: 7.5041\n",
            "English Pretraining Epoch 1/5, Batch 310, Loss: 7.7061\n",
            "English Pretraining Epoch 1/5, Batch 320, Loss: 8.3397\n",
            "English Pretraining Epoch 1/5, Batch 330, Loss: 7.3822\n",
            "English Pretraining Epoch 1/5, Batch 340, Loss: 7.6880\n",
            "English Pretraining Epoch 1/5, Batch 350, Loss: 7.4348\n",
            "English Pretraining Epoch 1/5, Batch 360, Loss: 7.2758\n",
            "English Pretraining Epoch 1/5, Batch 370, Loss: 7.7303\n",
            "English Pretraining Epoch 1/5, Batch 380, Loss: 7.7370\n",
            "English Pretraining Epoch 1/5, Batch 390, Loss: 7.4213\n",
            "English Pretraining Epoch 1/5, Batch 400, Loss: 8.4587\n",
            "English Pretraining Epoch 1/5, Batch 410, Loss: 7.8022\n",
            "English Pretraining Epoch 1/5, Batch 420, Loss: 7.6632\n",
            "English Pretraining Epoch 1/5, Batch 430, Loss: 7.4122\n",
            "English Pretraining Epoch 1/5, Batch 440, Loss: 7.6750\n",
            "English Pretraining Epoch 1/5, Batch 450, Loss: 7.4939\n",
            "English Pretraining Epoch 1/5, Batch 460, Loss: 7.6395\n",
            "English Pretraining Epoch 1/5, Batch 470, Loss: 7.1823\n",
            "English Pretraining Epoch 1/5, Batch 480, Loss: 7.8302\n",
            "English Pretraining Epoch 1/5, Batch 490, Loss: 7.6552\n",
            "English Pretraining Epoch 1/5, Batch 500, Loss: 7.7446\n",
            "English Pretraining Epoch 1/5, Batch 510, Loss: 7.7129\n",
            "English Pretraining Epoch 1/5, Batch 520, Loss: 7.6026\n",
            "English Pretraining Epoch 1/5, Batch 530, Loss: 7.3056\n",
            "English Pretraining Epoch 1/5, Batch 540, Loss: 7.3623\n",
            "English Pretraining Epoch 1/5, Batch 550, Loss: 7.5378\n",
            "English Pretraining Epoch 1/5, Batch 560, Loss: 7.5760\n",
            "English Pretraining Epoch 1/5, Batch 570, Loss: 7.4317\n",
            "English Pretraining Epoch 1/5, Batch 580, Loss: 7.4541\n",
            "English Pretraining Epoch 1/5, Batch 590, Loss: 7.3706\n",
            "English Pretraining Epoch 1/5, Batch 600, Loss: 7.1426\n",
            "English Pretraining Epoch 1/5, Batch 610, Loss: 7.6262\n",
            "English Pretraining Epoch 1/5, Batch 620, Loss: 7.0874\n",
            "✅ English Pretraining Epoch 1 completed, Average Loss: 7.7575\n",
            "English Pretraining Epoch 2/5, Batch 10, Loss: 6.9458\n",
            "English Pretraining Epoch 2/5, Batch 20, Loss: 7.1087\n",
            "English Pretraining Epoch 2/5, Batch 30, Loss: 7.0642\n",
            "English Pretraining Epoch 2/5, Batch 40, Loss: 7.1021\n",
            "English Pretraining Epoch 2/5, Batch 50, Loss: 6.9546\n",
            "English Pretraining Epoch 2/5, Batch 60, Loss: 6.7545\n",
            "English Pretraining Epoch 2/5, Batch 70, Loss: 7.0972\n",
            "English Pretraining Epoch 2/5, Batch 80, Loss: 6.9949\n",
            "English Pretraining Epoch 2/5, Batch 90, Loss: 7.0052\n",
            "English Pretraining Epoch 2/5, Batch 100, Loss: 7.0551\n",
            "English Pretraining Epoch 2/5, Batch 110, Loss: 7.4074\n",
            "English Pretraining Epoch 2/5, Batch 120, Loss: 6.5388\n",
            "English Pretraining Epoch 2/5, Batch 130, Loss: 7.0772\n",
            "English Pretraining Epoch 2/5, Batch 140, Loss: 6.9032\n",
            "English Pretraining Epoch 2/5, Batch 150, Loss: 6.7702\n",
            "English Pretraining Epoch 2/5, Batch 160, Loss: 6.9814\n",
            "English Pretraining Epoch 2/5, Batch 170, Loss: 7.2723\n",
            "English Pretraining Epoch 2/5, Batch 180, Loss: 6.9004\n",
            "English Pretraining Epoch 2/5, Batch 190, Loss: 6.9610\n",
            "English Pretraining Epoch 2/5, Batch 200, Loss: 6.8681\n",
            "English Pretraining Epoch 2/5, Batch 210, Loss: 6.9728\n",
            "English Pretraining Epoch 2/5, Batch 220, Loss: 7.0771\n",
            "English Pretraining Epoch 2/5, Batch 230, Loss: 6.8010\n",
            "English Pretraining Epoch 2/5, Batch 240, Loss: 6.7836\n",
            "English Pretraining Epoch 2/5, Batch 250, Loss: 6.7155\n",
            "English Pretraining Epoch 2/5, Batch 260, Loss: 6.6298\n",
            "English Pretraining Epoch 2/5, Batch 270, Loss: 6.6348\n",
            "English Pretraining Epoch 2/5, Batch 280, Loss: 6.6891\n",
            "English Pretraining Epoch 2/5, Batch 290, Loss: 6.3123\n",
            "English Pretraining Epoch 2/5, Batch 300, Loss: 6.8905\n",
            "English Pretraining Epoch 2/5, Batch 310, Loss: 6.7627\n",
            "English Pretraining Epoch 2/5, Batch 320, Loss: 7.4443\n",
            "English Pretraining Epoch 2/5, Batch 330, Loss: 6.5307\n",
            "English Pretraining Epoch 2/5, Batch 340, Loss: 6.7762\n",
            "English Pretraining Epoch 2/5, Batch 350, Loss: 6.5789\n",
            "English Pretraining Epoch 2/5, Batch 360, Loss: 6.4987\n",
            "English Pretraining Epoch 2/5, Batch 370, Loss: 6.9176\n",
            "English Pretraining Epoch 2/5, Batch 380, Loss: 6.7737\n",
            "English Pretraining Epoch 2/5, Batch 390, Loss: 6.3716\n",
            "English Pretraining Epoch 2/5, Batch 400, Loss: 7.4377\n",
            "English Pretraining Epoch 2/5, Batch 410, Loss: 6.9192\n",
            "English Pretraining Epoch 2/5, Batch 420, Loss: 6.7907\n",
            "English Pretraining Epoch 2/5, Batch 430, Loss: 6.5171\n",
            "English Pretraining Epoch 2/5, Batch 440, Loss: 6.8110\n",
            "English Pretraining Epoch 2/5, Batch 450, Loss: 6.5532\n",
            "English Pretraining Epoch 2/5, Batch 460, Loss: 6.7122\n",
            "English Pretraining Epoch 2/5, Batch 470, Loss: 6.2468\n",
            "English Pretraining Epoch 2/5, Batch 480, Loss: 6.9139\n",
            "English Pretraining Epoch 2/5, Batch 490, Loss: 6.7472\n",
            "English Pretraining Epoch 2/5, Batch 500, Loss: 6.8776\n",
            "English Pretraining Epoch 2/5, Batch 510, Loss: 6.8373\n",
            "English Pretraining Epoch 2/5, Batch 520, Loss: 6.7433\n",
            "English Pretraining Epoch 2/5, Batch 530, Loss: 6.4931\n",
            "English Pretraining Epoch 2/5, Batch 540, Loss: 6.6542\n",
            "English Pretraining Epoch 2/5, Batch 550, Loss: 6.8210\n",
            "English Pretraining Epoch 2/5, Batch 560, Loss: 6.7035\n",
            "English Pretraining Epoch 2/5, Batch 570, Loss: 6.5717\n",
            "English Pretraining Epoch 2/5, Batch 580, Loss: 6.7742\n",
            "English Pretraining Epoch 2/5, Batch 590, Loss: 6.5699\n",
            "English Pretraining Epoch 2/5, Batch 600, Loss: 6.3277\n",
            "English Pretraining Epoch 2/5, Batch 610, Loss: 6.8062\n",
            "English Pretraining Epoch 2/5, Batch 620, Loss: 6.4660\n",
            "✅ English Pretraining Epoch 2 completed, Average Loss: 6.7988\n",
            "English Pretraining Epoch 3/5, Batch 10, Loss: 6.1823\n",
            "English Pretraining Epoch 3/5, Batch 20, Loss: 6.5865\n",
            "English Pretraining Epoch 3/5, Batch 30, Loss: 6.1966\n",
            "English Pretraining Epoch 3/5, Batch 40, Loss: 6.4786\n",
            "English Pretraining Epoch 3/5, Batch 50, Loss: 6.3277\n",
            "English Pretraining Epoch 3/5, Batch 60, Loss: 5.9686\n",
            "English Pretraining Epoch 3/5, Batch 70, Loss: 6.3940\n",
            "English Pretraining Epoch 3/5, Batch 80, Loss: 6.4414\n",
            "English Pretraining Epoch 3/5, Batch 90, Loss: 6.3397\n",
            "English Pretraining Epoch 3/5, Batch 100, Loss: 6.4621\n",
            "English Pretraining Epoch 3/5, Batch 110, Loss: 6.8550\n",
            "English Pretraining Epoch 3/5, Batch 120, Loss: 6.0098\n",
            "English Pretraining Epoch 3/5, Batch 130, Loss: 6.6380\n",
            "English Pretraining Epoch 3/5, Batch 140, Loss: 6.2797\n",
            "English Pretraining Epoch 3/5, Batch 150, Loss: 6.2159\n",
            "English Pretraining Epoch 3/5, Batch 160, Loss: 6.6034\n",
            "English Pretraining Epoch 3/5, Batch 170, Loss: 6.8814\n",
            "English Pretraining Epoch 3/5, Batch 180, Loss: 6.4590\n",
            "English Pretraining Epoch 3/5, Batch 190, Loss: 6.5864\n",
            "English Pretraining Epoch 3/5, Batch 200, Loss: 6.4230\n",
            "English Pretraining Epoch 3/5, Batch 210, Loss: 6.6082\n",
            "English Pretraining Epoch 3/5, Batch 220, Loss: 6.6849\n",
            "English Pretraining Epoch 3/5, Batch 230, Loss: 6.4130\n",
            "English Pretraining Epoch 3/5, Batch 240, Loss: 6.4470\n",
            "English Pretraining Epoch 3/5, Batch 250, Loss: 6.3658\n",
            "English Pretraining Epoch 3/5, Batch 260, Loss: 6.2790\n",
            "English Pretraining Epoch 3/5, Batch 270, Loss: 6.3269\n",
            "English Pretraining Epoch 3/5, Batch 280, Loss: 6.3945\n",
            "English Pretraining Epoch 3/5, Batch 290, Loss: 5.9288\n",
            "English Pretraining Epoch 3/5, Batch 300, Loss: 6.6008\n",
            "English Pretraining Epoch 3/5, Batch 310, Loss: 6.4628\n",
            "English Pretraining Epoch 3/5, Batch 320, Loss: 7.1240\n",
            "English Pretraining Epoch 3/5, Batch 330, Loss: 6.2465\n",
            "English Pretraining Epoch 3/5, Batch 340, Loss: 6.4342\n",
            "English Pretraining Epoch 3/5, Batch 350, Loss: 6.2777\n",
            "English Pretraining Epoch 3/5, Batch 360, Loss: 6.2447\n",
            "English Pretraining Epoch 3/5, Batch 370, Loss: 6.6027\n",
            "English Pretraining Epoch 3/5, Batch 380, Loss: 6.4426\n",
            "English Pretraining Epoch 3/5, Batch 390, Loss: 5.9672\n",
            "English Pretraining Epoch 3/5, Batch 400, Loss: 7.0160\n",
            "English Pretraining Epoch 3/5, Batch 410, Loss: 6.6627\n",
            "English Pretraining Epoch 3/5, Batch 420, Loss: 6.5165\n",
            "English Pretraining Epoch 3/5, Batch 430, Loss: 6.2166\n",
            "English Pretraining Epoch 3/5, Batch 440, Loss: 6.5205\n",
            "English Pretraining Epoch 3/5, Batch 450, Loss: 6.2806\n",
            "English Pretraining Epoch 3/5, Batch 460, Loss: 6.4169\n",
            "English Pretraining Epoch 3/5, Batch 470, Loss: 6.0148\n",
            "English Pretraining Epoch 3/5, Batch 480, Loss: 6.6401\n",
            "English Pretraining Epoch 3/5, Batch 490, Loss: 6.4868\n",
            "English Pretraining Epoch 3/5, Batch 500, Loss: 6.6378\n",
            "English Pretraining Epoch 3/5, Batch 510, Loss: 6.6042\n",
            "English Pretraining Epoch 3/5, Batch 520, Loss: 6.4892\n",
            "English Pretraining Epoch 3/5, Batch 530, Loss: 6.2526\n",
            "English Pretraining Epoch 3/5, Batch 540, Loss: 6.3953\n",
            "English Pretraining Epoch 3/5, Batch 550, Loss: 6.5122\n",
            "English Pretraining Epoch 3/5, Batch 560, Loss: 6.4078\n",
            "English Pretraining Epoch 3/5, Batch 570, Loss: 6.2528\n",
            "English Pretraining Epoch 3/5, Batch 580, Loss: 6.5313\n",
            "English Pretraining Epoch 3/5, Batch 590, Loss: 6.3645\n",
            "English Pretraining Epoch 3/5, Batch 600, Loss: 6.0940\n",
            "English Pretraining Epoch 3/5, Batch 610, Loss: 6.5498\n",
            "English Pretraining Epoch 3/5, Batch 620, Loss: 6.2039\n",
            "✅ English Pretraining Epoch 3 completed, Average Loss: 6.4235\n",
            "English Pretraining Epoch 4/5, Batch 10, Loss: 6.0027\n",
            "English Pretraining Epoch 4/5, Batch 20, Loss: 6.4373\n",
            "English Pretraining Epoch 4/5, Batch 30, Loss: 5.9601\n",
            "English Pretraining Epoch 4/5, Batch 40, Loss: 6.2750\n",
            "English Pretraining Epoch 4/5, Batch 50, Loss: 6.1823\n",
            "English Pretraining Epoch 4/5, Batch 60, Loss: 5.7509\n",
            "English Pretraining Epoch 4/5, Batch 70, Loss: 6.1379\n",
            "English Pretraining Epoch 4/5, Batch 80, Loss: 6.2409\n",
            "English Pretraining Epoch 4/5, Batch 90, Loss: 6.1508\n",
            "English Pretraining Epoch 4/5, Batch 100, Loss: 6.2739\n",
            "English Pretraining Epoch 4/5, Batch 110, Loss: 6.6793\n",
            "English Pretraining Epoch 4/5, Batch 120, Loss: 5.8157\n",
            "English Pretraining Epoch 4/5, Batch 130, Loss: 6.5015\n",
            "English Pretraining Epoch 4/5, Batch 140, Loss: 6.0634\n",
            "English Pretraining Epoch 4/5, Batch 150, Loss: 5.9251\n",
            "English Pretraining Epoch 4/5, Batch 160, Loss: 6.4059\n",
            "English Pretraining Epoch 4/5, Batch 170, Loss: 6.7177\n",
            "English Pretraining Epoch 4/5, Batch 180, Loss: 6.2578\n",
            "English Pretraining Epoch 4/5, Batch 190, Loss: 6.3914\n",
            "English Pretraining Epoch 4/5, Batch 200, Loss: 6.2161\n",
            "English Pretraining Epoch 4/5, Batch 210, Loss: 6.3940\n",
            "English Pretraining Epoch 4/5, Batch 220, Loss: 6.4739\n",
            "English Pretraining Epoch 4/5, Batch 230, Loss: 6.2283\n",
            "English Pretraining Epoch 4/5, Batch 240, Loss: 6.3047\n",
            "English Pretraining Epoch 4/5, Batch 250, Loss: 6.2255\n",
            "English Pretraining Epoch 4/5, Batch 260, Loss: 6.0936\n",
            "English Pretraining Epoch 4/5, Batch 270, Loss: 6.1921\n",
            "English Pretraining Epoch 4/5, Batch 280, Loss: 6.1838\n",
            "English Pretraining Epoch 4/5, Batch 290, Loss: 5.7487\n",
            "English Pretraining Epoch 4/5, Batch 300, Loss: 6.4053\n",
            "English Pretraining Epoch 4/5, Batch 310, Loss: 6.3212\n",
            "English Pretraining Epoch 4/5, Batch 320, Loss: 6.9589\n",
            "English Pretraining Epoch 4/5, Batch 330, Loss: 6.0700\n",
            "English Pretraining Epoch 4/5, Batch 340, Loss: 6.2934\n",
            "English Pretraining Epoch 4/5, Batch 350, Loss: 6.0663\n",
            "English Pretraining Epoch 4/5, Batch 360, Loss: 6.0872\n",
            "English Pretraining Epoch 4/5, Batch 370, Loss: 6.4402\n",
            "English Pretraining Epoch 4/5, Batch 380, Loss: 6.2845\n",
            "English Pretraining Epoch 4/5, Batch 390, Loss: 5.7448\n",
            "English Pretraining Epoch 4/5, Batch 400, Loss: 6.7899\n",
            "English Pretraining Epoch 4/5, Batch 410, Loss: 6.5159\n",
            "English Pretraining Epoch 4/5, Batch 420, Loss: 6.3836\n",
            "English Pretraining Epoch 4/5, Batch 430, Loss: 6.0803\n",
            "English Pretraining Epoch 4/5, Batch 440, Loss: 6.3774\n",
            "English Pretraining Epoch 4/5, Batch 450, Loss: 6.0909\n",
            "English Pretraining Epoch 4/5, Batch 460, Loss: 6.2043\n",
            "English Pretraining Epoch 4/5, Batch 470, Loss: 5.8905\n",
            "English Pretraining Epoch 4/5, Batch 480, Loss: 6.4530\n",
            "English Pretraining Epoch 4/5, Batch 490, Loss: 6.3311\n",
            "English Pretraining Epoch 4/5, Batch 500, Loss: 6.4810\n",
            "English Pretraining Epoch 4/5, Batch 510, Loss: 6.4123\n",
            "English Pretraining Epoch 4/5, Batch 520, Loss: 6.3452\n",
            "English Pretraining Epoch 4/5, Batch 530, Loss: 6.0787\n",
            "English Pretraining Epoch 4/5, Batch 540, Loss: 6.2277\n",
            "English Pretraining Epoch 4/5, Batch 550, Loss: 6.2839\n",
            "English Pretraining Epoch 4/5, Batch 560, Loss: 6.2321\n",
            "English Pretraining Epoch 4/5, Batch 570, Loss: 6.0227\n",
            "English Pretraining Epoch 4/5, Batch 580, Loss: 6.3535\n",
            "English Pretraining Epoch 4/5, Batch 590, Loss: 6.1888\n",
            "English Pretraining Epoch 4/5, Batch 600, Loss: 5.9557\n",
            "English Pretraining Epoch 4/5, Batch 610, Loss: 6.3624\n",
            "English Pretraining Epoch 4/5, Batch 620, Loss: 6.0366\n",
            "✅ English Pretraining Epoch 4 completed, Average Loss: 6.2484\n",
            "English Pretraining Epoch 5/5, Batch 10, Loss: 5.9147\n",
            "English Pretraining Epoch 5/5, Batch 20, Loss: 6.3086\n",
            "English Pretraining Epoch 5/5, Batch 30, Loss: 5.8155\n",
            "English Pretraining Epoch 5/5, Batch 40, Loss: 6.1331\n",
            "English Pretraining Epoch 5/5, Batch 50, Loss: 6.0825\n",
            "English Pretraining Epoch 5/5, Batch 60, Loss: 5.5809\n",
            "English Pretraining Epoch 5/5, Batch 70, Loss: 5.9664\n",
            "English Pretraining Epoch 5/5, Batch 80, Loss: 6.0824\n",
            "English Pretraining Epoch 5/5, Batch 90, Loss: 5.9891\n",
            "English Pretraining Epoch 5/5, Batch 100, Loss: 6.1595\n",
            "English Pretraining Epoch 5/5, Batch 110, Loss: 6.5778\n",
            "English Pretraining Epoch 5/5, Batch 120, Loss: 5.6694\n",
            "English Pretraining Epoch 5/5, Batch 130, Loss: 6.3933\n",
            "English Pretraining Epoch 5/5, Batch 140, Loss: 5.9263\n",
            "English Pretraining Epoch 5/5, Batch 150, Loss: 5.7510\n",
            "English Pretraining Epoch 5/5, Batch 160, Loss: 6.2519\n",
            "English Pretraining Epoch 5/5, Batch 170, Loss: 6.5911\n",
            "English Pretraining Epoch 5/5, Batch 180, Loss: 6.0980\n",
            "English Pretraining Epoch 5/5, Batch 190, Loss: 6.2316\n",
            "English Pretraining Epoch 5/5, Batch 200, Loss: 6.0326\n",
            "English Pretraining Epoch 5/5, Batch 210, Loss: 6.2748\n",
            "English Pretraining Epoch 5/5, Batch 220, Loss: 6.3361\n",
            "English Pretraining Epoch 5/5, Batch 230, Loss: 6.0792\n",
            "English Pretraining Epoch 5/5, Batch 240, Loss: 6.1756\n",
            "English Pretraining Epoch 5/5, Batch 250, Loss: 6.1159\n",
            "English Pretraining Epoch 5/5, Batch 260, Loss: 5.9698\n",
            "English Pretraining Epoch 5/5, Batch 270, Loss: 6.0938\n",
            "English Pretraining Epoch 5/5, Batch 280, Loss: 6.0760\n",
            "English Pretraining Epoch 5/5, Batch 290, Loss: 5.6325\n",
            "English Pretraining Epoch 5/5, Batch 300, Loss: 6.2743\n",
            "English Pretraining Epoch 5/5, Batch 310, Loss: 6.2084\n",
            "English Pretraining Epoch 5/5, Batch 320, Loss: 6.8334\n",
            "English Pretraining Epoch 5/5, Batch 330, Loss: 5.9487\n",
            "English Pretraining Epoch 5/5, Batch 340, Loss: 6.1337\n",
            "English Pretraining Epoch 5/5, Batch 350, Loss: 5.9651\n",
            "English Pretraining Epoch 5/5, Batch 360, Loss: 5.9564\n",
            "English Pretraining Epoch 5/5, Batch 370, Loss: 6.3274\n",
            "English Pretraining Epoch 5/5, Batch 380, Loss: 6.1364\n",
            "English Pretraining Epoch 5/5, Batch 390, Loss: 5.6064\n",
            "English Pretraining Epoch 5/5, Batch 400, Loss: 6.6075\n",
            "English Pretraining Epoch 5/5, Batch 410, Loss: 6.3988\n",
            "English Pretraining Epoch 5/5, Batch 420, Loss: 6.2786\n",
            "English Pretraining Epoch 5/5, Batch 430, Loss: 5.9505\n",
            "English Pretraining Epoch 5/5, Batch 440, Loss: 6.2439\n",
            "English Pretraining Epoch 5/5, Batch 450, Loss: 5.9623\n",
            "English Pretraining Epoch 5/5, Batch 460, Loss: 6.0450\n",
            "English Pretraining Epoch 5/5, Batch 470, Loss: 5.7895\n",
            "English Pretraining Epoch 5/5, Batch 480, Loss: 6.3082\n",
            "English Pretraining Epoch 5/5, Batch 490, Loss: 6.2079\n",
            "English Pretraining Epoch 5/5, Batch 500, Loss: 6.3600\n",
            "English Pretraining Epoch 5/5, Batch 510, Loss: 6.2972\n",
            "English Pretraining Epoch 5/5, Batch 520, Loss: 6.2110\n",
            "English Pretraining Epoch 5/5, Batch 530, Loss: 5.9480\n",
            "English Pretraining Epoch 5/5, Batch 540, Loss: 6.1462\n",
            "English Pretraining Epoch 5/5, Batch 550, Loss: 6.1254\n",
            "English Pretraining Epoch 5/5, Batch 560, Loss: 6.0763\n",
            "English Pretraining Epoch 5/5, Batch 570, Loss: 5.8417\n",
            "English Pretraining Epoch 5/5, Batch 580, Loss: 6.1837\n",
            "English Pretraining Epoch 5/5, Batch 590, Loss: 6.0557\n",
            "English Pretraining Epoch 5/5, Batch 600, Loss: 5.8423\n",
            "English Pretraining Epoch 5/5, Batch 610, Loss: 6.2350\n",
            "English Pretraining Epoch 5/5, Batch 620, Loss: 5.8863\n",
            "✅ English Pretraining Epoch 5 completed, Average Loss: 6.1173\n",
            "🎉 English Pretraining completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n📘 Fine-tuning on Domain Paragraphs...\")\n",
        "pretrain_language_model(model, finetune_data, tokenizer, epochs=60, lr=3e-5, stage_name=\"Domain Fine-tuning\",batch_size=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx06ER_f8EyK",
        "outputId": "4a2f8c05-a6b5-4819-917c-ee4592220c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📘 Fine-tuning on Domain Paragraphs...\n",
            "🚀 Domain Fine-tuning for 60 epochs with lr=3e-05...\n",
            "Domain Fine-tuning Epoch 1/60, Batch 10, Loss: 9.0222\n",
            "Domain Fine-tuning Epoch 1/60, Batch 20, Loss: 8.7238\n",
            "Domain Fine-tuning Epoch 1/60, Batch 30, Loss: 7.8060\n",
            "✅ Domain Fine-tuning Epoch 1 completed, Average Loss: 8.6102\n",
            "Domain Fine-tuning Epoch 2/60, Batch 10, Loss: 7.3342\n",
            "Domain Fine-tuning Epoch 2/60, Batch 20, Loss: 7.2530\n",
            "Domain Fine-tuning Epoch 2/60, Batch 30, Loss: 6.8837\n",
            "✅ Domain Fine-tuning Epoch 2 completed, Average Loss: 7.1024\n",
            "Domain Fine-tuning Epoch 3/60, Batch 10, Loss: 6.7322\n",
            "Domain Fine-tuning Epoch 3/60, Batch 20, Loss: 6.5442\n",
            "Domain Fine-tuning Epoch 3/60, Batch 30, Loss: 6.4743\n",
            "✅ Domain Fine-tuning Epoch 3 completed, Average Loss: 6.5581\n",
            "Domain Fine-tuning Epoch 4/60, Batch 10, Loss: 6.3100\n",
            "Domain Fine-tuning Epoch 4/60, Batch 20, Loss: 6.2545\n",
            "Domain Fine-tuning Epoch 4/60, Batch 30, Loss: 6.1312\n",
            "✅ Domain Fine-tuning Epoch 4 completed, Average Loss: 6.2328\n",
            "Domain Fine-tuning Epoch 5/60, Batch 10, Loss: 5.9931\n",
            "Domain Fine-tuning Epoch 5/60, Batch 20, Loss: 6.0553\n",
            "Domain Fine-tuning Epoch 5/60, Batch 30, Loss: 5.8125\n",
            "✅ Domain Fine-tuning Epoch 5 completed, Average Loss: 5.9960\n",
            "Domain Fine-tuning Epoch 6/60, Batch 10, Loss: 5.8261\n",
            "Domain Fine-tuning Epoch 6/60, Batch 20, Loss: 5.7581\n",
            "Domain Fine-tuning Epoch 6/60, Batch 30, Loss: 5.6466\n",
            "✅ Domain Fine-tuning Epoch 6 completed, Average Loss: 5.8088\n",
            "Domain Fine-tuning Epoch 7/60, Batch 10, Loss: 5.6333\n",
            "Domain Fine-tuning Epoch 7/60, Batch 20, Loss: 5.6674\n",
            "Domain Fine-tuning Epoch 7/60, Batch 30, Loss: 5.5391\n",
            "✅ Domain Fine-tuning Epoch 7 completed, Average Loss: 5.6489\n",
            "Domain Fine-tuning Epoch 8/60, Batch 10, Loss: 5.5193\n",
            "Domain Fine-tuning Epoch 8/60, Batch 20, Loss: 5.4699\n",
            "Domain Fine-tuning Epoch 8/60, Batch 30, Loss: 5.4052\n",
            "✅ Domain Fine-tuning Epoch 8 completed, Average Loss: 5.5112\n",
            "Domain Fine-tuning Epoch 9/60, Batch 10, Loss: 5.3957\n",
            "Domain Fine-tuning Epoch 9/60, Batch 20, Loss: 5.3836\n",
            "Domain Fine-tuning Epoch 9/60, Batch 30, Loss: 5.2863\n",
            "✅ Domain Fine-tuning Epoch 9 completed, Average Loss: 5.3766\n",
            "Domain Fine-tuning Epoch 10/60, Batch 10, Loss: 5.2027\n",
            "Domain Fine-tuning Epoch 10/60, Batch 20, Loss: 5.2679\n",
            "Domain Fine-tuning Epoch 10/60, Batch 30, Loss: 5.2132\n",
            "✅ Domain Fine-tuning Epoch 10 completed, Average Loss: 5.2582\n",
            "Domain Fine-tuning Epoch 11/60, Batch 10, Loss: 5.1929\n",
            "Domain Fine-tuning Epoch 11/60, Batch 20, Loss: 5.1724\n",
            "Domain Fine-tuning Epoch 11/60, Batch 30, Loss: 5.0989\n",
            "✅ Domain Fine-tuning Epoch 11 completed, Average Loss: 5.1634\n",
            "Domain Fine-tuning Epoch 12/60, Batch 10, Loss: 5.0929\n",
            "Domain Fine-tuning Epoch 12/60, Batch 20, Loss: 5.0190\n",
            "Domain Fine-tuning Epoch 12/60, Batch 30, Loss: 4.9860\n",
            "✅ Domain Fine-tuning Epoch 12 completed, Average Loss: 5.0637\n",
            "Domain Fine-tuning Epoch 13/60, Batch 10, Loss: 5.0874\n",
            "Domain Fine-tuning Epoch 13/60, Batch 20, Loss: 4.9169\n",
            "Domain Fine-tuning Epoch 13/60, Batch 30, Loss: 4.9127\n",
            "✅ Domain Fine-tuning Epoch 13 completed, Average Loss: 4.9686\n",
            "Domain Fine-tuning Epoch 14/60, Batch 10, Loss: 4.9339\n",
            "Domain Fine-tuning Epoch 14/60, Batch 20, Loss: 4.8566\n",
            "Domain Fine-tuning Epoch 14/60, Batch 30, Loss: 4.7981\n",
            "✅ Domain Fine-tuning Epoch 14 completed, Average Loss: 4.8709\n",
            "Domain Fine-tuning Epoch 15/60, Batch 10, Loss: 4.8037\n",
            "Domain Fine-tuning Epoch 15/60, Batch 20, Loss: 4.7536\n",
            "Domain Fine-tuning Epoch 15/60, Batch 30, Loss: 4.7707\n",
            "✅ Domain Fine-tuning Epoch 15 completed, Average Loss: 4.7736\n",
            "Domain Fine-tuning Epoch 16/60, Batch 10, Loss: 4.6843\n",
            "Domain Fine-tuning Epoch 16/60, Batch 20, Loss: 4.6435\n",
            "Domain Fine-tuning Epoch 16/60, Batch 30, Loss: 4.7007\n",
            "✅ Domain Fine-tuning Epoch 16 completed, Average Loss: 4.6652\n",
            "Domain Fine-tuning Epoch 17/60, Batch 10, Loss: 4.6965\n",
            "Domain Fine-tuning Epoch 17/60, Batch 20, Loss: 4.5943\n",
            "Domain Fine-tuning Epoch 17/60, Batch 30, Loss: 4.6414\n",
            "✅ Domain Fine-tuning Epoch 17 completed, Average Loss: 4.5810\n",
            "Domain Fine-tuning Epoch 18/60, Batch 10, Loss: 4.6385\n",
            "Domain Fine-tuning Epoch 18/60, Batch 20, Loss: 4.4908\n",
            "Domain Fine-tuning Epoch 18/60, Batch 30, Loss: 4.5448\n",
            "✅ Domain Fine-tuning Epoch 18 completed, Average Loss: 4.5001\n",
            "Domain Fine-tuning Epoch 19/60, Batch 10, Loss: 4.4869\n",
            "Domain Fine-tuning Epoch 19/60, Batch 20, Loss: 4.4219\n",
            "Domain Fine-tuning Epoch 19/60, Batch 30, Loss: 4.4617\n",
            "✅ Domain Fine-tuning Epoch 19 completed, Average Loss: 4.3758\n",
            "Domain Fine-tuning Epoch 20/60, Batch 10, Loss: 4.4189\n",
            "Domain Fine-tuning Epoch 20/60, Batch 20, Loss: 4.2337\n",
            "Domain Fine-tuning Epoch 20/60, Batch 30, Loss: 4.4056\n",
            "✅ Domain Fine-tuning Epoch 20 completed, Average Loss: 4.2965\n",
            "Domain Fine-tuning Epoch 21/60, Batch 10, Loss: 4.3274\n",
            "Domain Fine-tuning Epoch 21/60, Batch 20, Loss: 4.1802\n",
            "Domain Fine-tuning Epoch 21/60, Batch 30, Loss: 4.3909\n",
            "✅ Domain Fine-tuning Epoch 21 completed, Average Loss: 4.2171\n",
            "Domain Fine-tuning Epoch 22/60, Batch 10, Loss: 4.2937\n",
            "Domain Fine-tuning Epoch 22/60, Batch 20, Loss: 4.1477\n",
            "Domain Fine-tuning Epoch 22/60, Batch 30, Loss: 4.2745\n",
            "✅ Domain Fine-tuning Epoch 22 completed, Average Loss: 4.1114\n",
            "Domain Fine-tuning Epoch 23/60, Batch 10, Loss: 4.0824\n",
            "Domain Fine-tuning Epoch 23/60, Batch 20, Loss: 4.0134\n",
            "Domain Fine-tuning Epoch 23/60, Batch 30, Loss: 4.2672\n",
            "✅ Domain Fine-tuning Epoch 23 completed, Average Loss: 4.0123\n",
            "Domain Fine-tuning Epoch 24/60, Batch 10, Loss: 3.9688\n",
            "Domain Fine-tuning Epoch 24/60, Batch 20, Loss: 3.9261\n",
            "Domain Fine-tuning Epoch 24/60, Batch 30, Loss: 4.2587\n",
            "✅ Domain Fine-tuning Epoch 24 completed, Average Loss: 3.9135\n",
            "Domain Fine-tuning Epoch 25/60, Batch 10, Loss: 3.8340\n",
            "Domain Fine-tuning Epoch 25/60, Batch 20, Loss: 3.8049\n",
            "Domain Fine-tuning Epoch 25/60, Batch 30, Loss: 4.1555\n",
            "✅ Domain Fine-tuning Epoch 25 completed, Average Loss: 3.8100\n",
            "Domain Fine-tuning Epoch 26/60, Batch 10, Loss: 3.8311\n",
            "Domain Fine-tuning Epoch 26/60, Batch 20, Loss: 3.7190\n",
            "Domain Fine-tuning Epoch 26/60, Batch 30, Loss: 4.0252\n",
            "✅ Domain Fine-tuning Epoch 26 completed, Average Loss: 3.7123\n",
            "Domain Fine-tuning Epoch 27/60, Batch 10, Loss: 3.7207\n",
            "Domain Fine-tuning Epoch 27/60, Batch 20, Loss: 3.6426\n",
            "Domain Fine-tuning Epoch 27/60, Batch 30, Loss: 4.0357\n",
            "✅ Domain Fine-tuning Epoch 27 completed, Average Loss: 3.6021\n",
            "Domain Fine-tuning Epoch 28/60, Batch 10, Loss: 3.5770\n",
            "Domain Fine-tuning Epoch 28/60, Batch 20, Loss: 3.4996\n",
            "Domain Fine-tuning Epoch 28/60, Batch 30, Loss: 3.7758\n",
            "✅ Domain Fine-tuning Epoch 28 completed, Average Loss: 3.5049\n",
            "Domain Fine-tuning Epoch 29/60, Batch 10, Loss: 3.3296\n",
            "Domain Fine-tuning Epoch 29/60, Batch 20, Loss: 3.4865\n",
            "Domain Fine-tuning Epoch 29/60, Batch 30, Loss: 3.7461\n",
            "✅ Domain Fine-tuning Epoch 29 completed, Average Loss: 3.3919\n",
            "Domain Fine-tuning Epoch 30/60, Batch 10, Loss: 3.4106\n",
            "Domain Fine-tuning Epoch 30/60, Batch 20, Loss: 3.3632\n",
            "Domain Fine-tuning Epoch 30/60, Batch 30, Loss: 3.7119\n",
            "✅ Domain Fine-tuning Epoch 30 completed, Average Loss: 3.2947\n",
            "Domain Fine-tuning Epoch 31/60, Batch 10, Loss: 3.2230\n",
            "Domain Fine-tuning Epoch 31/60, Batch 20, Loss: 3.2384\n",
            "Domain Fine-tuning Epoch 31/60, Batch 30, Loss: 3.5123\n",
            "✅ Domain Fine-tuning Epoch 31 completed, Average Loss: 3.1997\n",
            "Domain Fine-tuning Epoch 32/60, Batch 10, Loss: 3.1380\n",
            "Domain Fine-tuning Epoch 32/60, Batch 20, Loss: 3.1684\n",
            "Domain Fine-tuning Epoch 32/60, Batch 30, Loss: 3.5077\n",
            "✅ Domain Fine-tuning Epoch 32 completed, Average Loss: 3.1095\n",
            "Domain Fine-tuning Epoch 33/60, Batch 10, Loss: 2.9600\n",
            "Domain Fine-tuning Epoch 33/60, Batch 20, Loss: 3.0155\n",
            "Domain Fine-tuning Epoch 33/60, Batch 30, Loss: 3.4348\n",
            "✅ Domain Fine-tuning Epoch 33 completed, Average Loss: 3.0077\n",
            "Domain Fine-tuning Epoch 34/60, Batch 10, Loss: 2.9313\n",
            "Domain Fine-tuning Epoch 34/60, Batch 20, Loss: 2.9374\n",
            "Domain Fine-tuning Epoch 34/60, Batch 30, Loss: 3.4974\n",
            "✅ Domain Fine-tuning Epoch 34 completed, Average Loss: 2.9081\n",
            "Domain Fine-tuning Epoch 35/60, Batch 10, Loss: 2.8694\n",
            "Domain Fine-tuning Epoch 35/60, Batch 20, Loss: 2.7667\n",
            "Domain Fine-tuning Epoch 35/60, Batch 30, Loss: 3.2997\n",
            "✅ Domain Fine-tuning Epoch 35 completed, Average Loss: 2.8210\n",
            "Domain Fine-tuning Epoch 36/60, Batch 10, Loss: 2.7659\n",
            "Domain Fine-tuning Epoch 36/60, Batch 20, Loss: 2.6928\n",
            "Domain Fine-tuning Epoch 36/60, Batch 30, Loss: 3.2207\n",
            "✅ Domain Fine-tuning Epoch 36 completed, Average Loss: 2.7058\n",
            "Domain Fine-tuning Epoch 37/60, Batch 10, Loss: 2.7270\n",
            "Domain Fine-tuning Epoch 37/60, Batch 20, Loss: 2.5889\n",
            "Domain Fine-tuning Epoch 37/60, Batch 30, Loss: 3.0975\n",
            "✅ Domain Fine-tuning Epoch 37 completed, Average Loss: 2.6237\n",
            "Domain Fine-tuning Epoch 38/60, Batch 10, Loss: 2.6350\n",
            "Domain Fine-tuning Epoch 38/60, Batch 20, Loss: 2.4689\n",
            "Domain Fine-tuning Epoch 38/60, Batch 30, Loss: 2.9368\n",
            "✅ Domain Fine-tuning Epoch 38 completed, Average Loss: 2.5246\n",
            "Domain Fine-tuning Epoch 39/60, Batch 10, Loss: 2.5656\n",
            "Domain Fine-tuning Epoch 39/60, Batch 20, Loss: 2.3812\n",
            "Domain Fine-tuning Epoch 39/60, Batch 30, Loss: 2.9424\n",
            "✅ Domain Fine-tuning Epoch 39 completed, Average Loss: 2.4573\n",
            "Domain Fine-tuning Epoch 40/60, Batch 10, Loss: 2.4244\n",
            "Domain Fine-tuning Epoch 40/60, Batch 20, Loss: 2.2671\n",
            "Domain Fine-tuning Epoch 40/60, Batch 30, Loss: 2.8919\n",
            "✅ Domain Fine-tuning Epoch 40 completed, Average Loss: 2.3588\n",
            "Domain Fine-tuning Epoch 41/60, Batch 10, Loss: 2.3749\n",
            "Domain Fine-tuning Epoch 41/60, Batch 20, Loss: 2.2754\n",
            "Domain Fine-tuning Epoch 41/60, Batch 30, Loss: 2.8052\n",
            "✅ Domain Fine-tuning Epoch 41 completed, Average Loss: 2.2876\n",
            "Domain Fine-tuning Epoch 42/60, Batch 10, Loss: 2.2103\n",
            "Domain Fine-tuning Epoch 42/60, Batch 20, Loss: 2.1727\n",
            "Domain Fine-tuning Epoch 42/60, Batch 30, Loss: 2.7043\n",
            "✅ Domain Fine-tuning Epoch 42 completed, Average Loss: 2.2323\n",
            "Domain Fine-tuning Epoch 43/60, Batch 10, Loss: 2.2048\n",
            "Domain Fine-tuning Epoch 43/60, Batch 20, Loss: 2.0350\n",
            "Domain Fine-tuning Epoch 43/60, Batch 30, Loss: 2.7135\n",
            "✅ Domain Fine-tuning Epoch 43 completed, Average Loss: 2.1417\n",
            "Domain Fine-tuning Epoch 44/60, Batch 10, Loss: 2.1764\n",
            "Domain Fine-tuning Epoch 44/60, Batch 20, Loss: 1.9955\n",
            "Domain Fine-tuning Epoch 44/60, Batch 30, Loss: 2.5797\n",
            "✅ Domain Fine-tuning Epoch 44 completed, Average Loss: 2.0836\n",
            "Domain Fine-tuning Epoch 45/60, Batch 10, Loss: 2.0787\n",
            "Domain Fine-tuning Epoch 45/60, Batch 20, Loss: 1.9417\n",
            "Domain Fine-tuning Epoch 45/60, Batch 30, Loss: 2.5629\n",
            "✅ Domain Fine-tuning Epoch 45 completed, Average Loss: 1.9900\n",
            "Domain Fine-tuning Epoch 46/60, Batch 10, Loss: 2.0628\n",
            "Domain Fine-tuning Epoch 46/60, Batch 20, Loss: 1.7991\n",
            "Domain Fine-tuning Epoch 46/60, Batch 30, Loss: 2.3086\n",
            "✅ Domain Fine-tuning Epoch 46 completed, Average Loss: 1.9197\n",
            "Domain Fine-tuning Epoch 47/60, Batch 10, Loss: 1.8618\n",
            "Domain Fine-tuning Epoch 47/60, Batch 20, Loss: 1.7252\n",
            "Domain Fine-tuning Epoch 47/60, Batch 30, Loss: 2.3002\n",
            "✅ Domain Fine-tuning Epoch 47 completed, Average Loss: 1.8595\n",
            "Domain Fine-tuning Epoch 48/60, Batch 10, Loss: 1.7710\n",
            "Domain Fine-tuning Epoch 48/60, Batch 20, Loss: 1.7510\n",
            "Domain Fine-tuning Epoch 48/60, Batch 30, Loss: 2.3127\n",
            "✅ Domain Fine-tuning Epoch 48 completed, Average Loss: 1.8098\n",
            "Domain Fine-tuning Epoch 49/60, Batch 10, Loss: 1.8101\n",
            "Domain Fine-tuning Epoch 49/60, Batch 20, Loss: 1.6551\n",
            "Domain Fine-tuning Epoch 49/60, Batch 30, Loss: 2.2228\n",
            "✅ Domain Fine-tuning Epoch 49 completed, Average Loss: 1.7480\n",
            "Domain Fine-tuning Epoch 50/60, Batch 10, Loss: 1.6814\n",
            "Domain Fine-tuning Epoch 50/60, Batch 20, Loss: 1.6558\n",
            "Domain Fine-tuning Epoch 50/60, Batch 30, Loss: 2.1711\n",
            "✅ Domain Fine-tuning Epoch 50 completed, Average Loss: 1.6832\n",
            "Domain Fine-tuning Epoch 51/60, Batch 10, Loss: 1.7309\n",
            "Domain Fine-tuning Epoch 51/60, Batch 20, Loss: 1.5104\n",
            "Domain Fine-tuning Epoch 51/60, Batch 30, Loss: 1.9809\n",
            "✅ Domain Fine-tuning Epoch 51 completed, Average Loss: 1.6189\n",
            "Domain Fine-tuning Epoch 52/60, Batch 10, Loss: 1.6568\n",
            "Domain Fine-tuning Epoch 52/60, Batch 20, Loss: 1.5063\n",
            "Domain Fine-tuning Epoch 52/60, Batch 30, Loss: 1.9129\n",
            "✅ Domain Fine-tuning Epoch 52 completed, Average Loss: 1.5671\n",
            "Domain Fine-tuning Epoch 53/60, Batch 10, Loss: 1.4949\n",
            "Domain Fine-tuning Epoch 53/60, Batch 20, Loss: 1.4396\n",
            "Domain Fine-tuning Epoch 53/60, Batch 30, Loss: 1.9426\n",
            "✅ Domain Fine-tuning Epoch 53 completed, Average Loss: 1.5336\n",
            "Domain Fine-tuning Epoch 54/60, Batch 10, Loss: 1.5794\n",
            "Domain Fine-tuning Epoch 54/60, Batch 20, Loss: 1.3425\n",
            "Domain Fine-tuning Epoch 54/60, Batch 30, Loss: 1.7867\n",
            "✅ Domain Fine-tuning Epoch 54 completed, Average Loss: 1.4703\n",
            "Domain Fine-tuning Epoch 55/60, Batch 10, Loss: 1.4974\n",
            "Domain Fine-tuning Epoch 55/60, Batch 20, Loss: 1.2827\n",
            "Domain Fine-tuning Epoch 55/60, Batch 30, Loss: 1.7476\n",
            "✅ Domain Fine-tuning Epoch 55 completed, Average Loss: 1.4219\n",
            "Domain Fine-tuning Epoch 56/60, Batch 10, Loss: 1.4860\n",
            "Domain Fine-tuning Epoch 56/60, Batch 20, Loss: 1.3090\n",
            "Domain Fine-tuning Epoch 56/60, Batch 30, Loss: 1.6786\n",
            "✅ Domain Fine-tuning Epoch 56 completed, Average Loss: 1.3789\n",
            "Domain Fine-tuning Epoch 57/60, Batch 10, Loss: 1.3859\n",
            "Domain Fine-tuning Epoch 57/60, Batch 20, Loss: 1.2169\n",
            "Domain Fine-tuning Epoch 57/60, Batch 30, Loss: 1.6331\n",
            "✅ Domain Fine-tuning Epoch 57 completed, Average Loss: 1.3501\n",
            "Domain Fine-tuning Epoch 58/60, Batch 10, Loss: 1.4562\n",
            "Domain Fine-tuning Epoch 58/60, Batch 20, Loss: 1.1609\n",
            "Domain Fine-tuning Epoch 58/60, Batch 30, Loss: 1.5537\n",
            "✅ Domain Fine-tuning Epoch 58 completed, Average Loss: 1.3100\n",
            "Domain Fine-tuning Epoch 59/60, Batch 10, Loss: 1.3855\n",
            "Domain Fine-tuning Epoch 59/60, Batch 20, Loss: 1.1369\n",
            "Domain Fine-tuning Epoch 59/60, Batch 30, Loss: 1.5348\n",
            "✅ Domain Fine-tuning Epoch 59 completed, Average Loss: 1.2624\n",
            "Domain Fine-tuning Epoch 60/60, Batch 10, Loss: 1.3374\n",
            "Domain Fine-tuning Epoch 60/60, Batch 20, Loss: 1.0519\n",
            "Domain Fine-tuning Epoch 60/60, Batch 30, Loss: 1.5030\n",
            "✅ Domain Fine-tuning Epoch 60 completed, Average Loss: 1.2352\n",
            "🎉 Domain Fine-tuning completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_qa(model, data, tokenizer, epochs=None, lr=None):\n",
        "    \"\"\"Fine-tuning function specifically for Q&A task (sequence-to-sequence)\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    print(f\"💬 Instruction Fine-tuning for {epochs} epochs with lr={lr}...\")\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        batch_size = 4  # Smaller batch size for Q&A\n",
        "\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch_qa = data[i:i+batch_size]\n",
        "\n",
        "            questions = []\n",
        "            answers = []\n",
        "\n",
        "            # Parse Q&A pairs\n",
        "            for qa_pair in batch_qa:\n",
        "                if \"Question:\" in qa_pair and \"Answer:\" in qa_pair:\n",
        "                    parts = qa_pair.split(\"Answer:\")\n",
        "                    if len(parts) == 2:\n",
        "                        q = parts[0].replace(\"Question:\", \"\").strip()\n",
        "                        a = parts[1].strip()\n",
        "                        questions.append(q)\n",
        "                        answers.append(a)\n",
        "\n",
        "            if not questions:\n",
        "                continue\n",
        "\n",
        "            # Tokenize questions (source) with proper special tokens\n",
        "            src_inputs = tokenizer(\n",
        "                questions,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=256,\n",
        "                return_tensors='pt',\n",
        "                add_special_tokens=True\n",
        "            )\n",
        "\n",
        "            # Tokenize answers (target) with proper special tokens\n",
        "            tgt_inputs = tokenizer(\n",
        "                answers,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=128,\n",
        "                return_tensors='pt',\n",
        "                add_special_tokens=True\n",
        "            )\n",
        "\n",
        "            src_ids = src_inputs['input_ids'].to(device)\n",
        "            src_mask = src_inputs['attention_mask'].to(device)\n",
        "            tgt_ids = tgt_inputs['input_ids'].to(device)\n",
        "\n",
        "            # Encode the source (questions)\n",
        "            memory = model.encode(src_ids, src_mask)\n",
        "\n",
        "            # Prepare target for decoder (teacher forcing)\n",
        "            decoder_input_ids = tgt_ids[:, :-1]  # Remove last token for input\n",
        "            labels = tgt_ids[:, 1:]  # Remove first token for labels\n",
        "\n",
        "            # Create causal mask for decoder\n",
        "            tgt_mask = model.generate_square_subsequent_mask(decoder_input_ids.size(1)).to(device)\n",
        "\n",
        "            # Decode to generate answers\n",
        "            logits = model.decode(\n",
        "                decoder_input_ids,\n",
        "                memory,\n",
        "                memory_mask=~src_mask.bool(),\n",
        "                tgt_mask=tgt_mask\n",
        "            )\n",
        "\n",
        "            # Use contiguous().view() to avoid stride issues\n",
        "            logits_flat = logits.contiguous().view(-1, logits.size(-1))\n",
        "            labels_flat = labels.contiguous().view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_function(logits_flat, labels_flat)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            if batch_count % 5 == 0:\n",
        "                print(f\" Q&A Epoch {epoch+1}/{epochs}, Batch {batch_count}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
        "        print(f\"✅ Q&A Epoch {epoch+1} completed, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"🎉 Instruction Fine-tuning completed successfully!\")"
      ],
      "metadata": {
        "id": "LUji0DOIlOrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n💬 Instruction Fine-tuning (Q&A)...\")\n",
        "fine_tune_qa(model, qa_data, tokenizer, epochs=50, lr=3e-5)\n",
        "\n",
        "torch.save(model.state_dict(), \"/content/corrected_model.pt\")\n",
        "print(\"💾 Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onRN0wVPE0Co",
        "outputId": "bbafdaad-2688-47e2-c937-e4035e210880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💬 Instruction Fine-tuning (Q&A)...\n",
            "💬 Instruction Fine-tuning for 50 epochs with lr=3e-05...\n",
            " Q&A Epoch 1/50, Batch 5, Loss: 7.5284\n",
            " Q&A Epoch 1/50, Batch 10, Loss: 6.2953\n",
            " Q&A Epoch 1/50, Batch 15, Loss: 6.1124\n",
            " Q&A Epoch 1/50, Batch 20, Loss: 5.7158\n",
            " Q&A Epoch 1/50, Batch 25, Loss: 5.6258\n",
            " Q&A Epoch 1/50, Batch 30, Loss: 5.4454\n",
            " Q&A Epoch 1/50, Batch 35, Loss: 6.1236\n",
            " Q&A Epoch 1/50, Batch 40, Loss: 5.7050\n",
            " Q&A Epoch 1/50, Batch 45, Loss: 6.0614\n",
            " Q&A Epoch 1/50, Batch 50, Loss: 5.7694\n",
            " Q&A Epoch 1/50, Batch 55, Loss: 5.6629\n",
            " Q&A Epoch 1/50, Batch 60, Loss: 4.7027\n",
            " Q&A Epoch 1/50, Batch 65, Loss: 5.2369\n",
            "✅ Q&A Epoch 1 completed, Average Loss: 5.8832\n",
            " Q&A Epoch 2/50, Batch 5, Loss: 5.4667\n",
            " Q&A Epoch 2/50, Batch 10, Loss: 4.9427\n",
            " Q&A Epoch 2/50, Batch 15, Loss: 5.0980\n",
            " Q&A Epoch 2/50, Batch 20, Loss: 5.1653\n",
            " Q&A Epoch 2/50, Batch 25, Loss: 5.1196\n",
            " Q&A Epoch 2/50, Batch 30, Loss: 4.9039\n",
            " Q&A Epoch 2/50, Batch 35, Loss: 5.6513\n",
            " Q&A Epoch 2/50, Batch 40, Loss: 5.2211\n",
            " Q&A Epoch 2/50, Batch 45, Loss: 5.6752\n",
            " Q&A Epoch 2/50, Batch 50, Loss: 5.4029\n",
            " Q&A Epoch 2/50, Batch 55, Loss: 5.2521\n",
            " Q&A Epoch 2/50, Batch 60, Loss: 4.4331\n",
            " Q&A Epoch 2/50, Batch 65, Loss: 4.8395\n",
            "✅ Q&A Epoch 2 completed, Average Loss: 5.0880\n",
            " Q&A Epoch 3/50, Batch 5, Loss: 5.1846\n",
            " Q&A Epoch 3/50, Batch 10, Loss: 4.5775\n",
            " Q&A Epoch 3/50, Batch 15, Loss: 4.7446\n",
            " Q&A Epoch 3/50, Batch 20, Loss: 4.7627\n",
            " Q&A Epoch 3/50, Batch 25, Loss: 4.7596\n",
            " Q&A Epoch 3/50, Batch 30, Loss: 4.6464\n",
            " Q&A Epoch 3/50, Batch 35, Loss: 5.3349\n",
            " Q&A Epoch 3/50, Batch 40, Loss: 4.9481\n",
            " Q&A Epoch 3/50, Batch 45, Loss: 5.4100\n",
            " Q&A Epoch 3/50, Batch 50, Loss: 5.1925\n",
            " Q&A Epoch 3/50, Batch 55, Loss: 5.0483\n",
            " Q&A Epoch 3/50, Batch 60, Loss: 4.2809\n",
            " Q&A Epoch 3/50, Batch 65, Loss: 4.6612\n",
            "✅ Q&A Epoch 3 completed, Average Loss: 4.8245\n",
            " Q&A Epoch 4/50, Batch 5, Loss: 5.0259\n",
            " Q&A Epoch 4/50, Batch 10, Loss: 4.3867\n",
            " Q&A Epoch 4/50, Batch 15, Loss: 4.4952\n",
            " Q&A Epoch 4/50, Batch 20, Loss: 4.6027\n",
            " Q&A Epoch 4/50, Batch 25, Loss: 4.6208\n",
            " Q&A Epoch 4/50, Batch 30, Loss: 4.4391\n",
            " Q&A Epoch 4/50, Batch 35, Loss: 5.0945\n",
            " Q&A Epoch 4/50, Batch 40, Loss: 4.6637\n",
            " Q&A Epoch 4/50, Batch 45, Loss: 5.1864\n",
            " Q&A Epoch 4/50, Batch 50, Loss: 4.9292\n",
            " Q&A Epoch 4/50, Batch 55, Loss: 4.8715\n",
            " Q&A Epoch 4/50, Batch 60, Loss: 4.1557\n",
            " Q&A Epoch 4/50, Batch 65, Loss: 4.5374\n",
            "✅ Q&A Epoch 4 completed, Average Loss: 4.6075\n",
            " Q&A Epoch 5/50, Batch 5, Loss: 4.8551\n",
            " Q&A Epoch 5/50, Batch 10, Loss: 4.1959\n",
            " Q&A Epoch 5/50, Batch 15, Loss: 4.3457\n",
            " Q&A Epoch 5/50, Batch 20, Loss: 4.3778\n",
            " Q&A Epoch 5/50, Batch 25, Loss: 4.3975\n",
            " Q&A Epoch 5/50, Batch 30, Loss: 4.1788\n",
            " Q&A Epoch 5/50, Batch 35, Loss: 4.9589\n",
            " Q&A Epoch 5/50, Batch 40, Loss: 4.4862\n",
            " Q&A Epoch 5/50, Batch 45, Loss: 5.0872\n",
            " Q&A Epoch 5/50, Batch 50, Loss: 4.7815\n",
            " Q&A Epoch 5/50, Batch 55, Loss: 4.6903\n",
            " Q&A Epoch 5/50, Batch 60, Loss: 3.9768\n",
            " Q&A Epoch 5/50, Batch 65, Loss: 4.2691\n",
            "✅ Q&A Epoch 5 completed, Average Loss: 4.4302\n",
            " Q&A Epoch 6/50, Batch 5, Loss: 4.6440\n",
            " Q&A Epoch 6/50, Batch 10, Loss: 4.1291\n",
            " Q&A Epoch 6/50, Batch 15, Loss: 4.1348\n",
            " Q&A Epoch 6/50, Batch 20, Loss: 4.2139\n",
            " Q&A Epoch 6/50, Batch 25, Loss: 4.3944\n",
            " Q&A Epoch 6/50, Batch 30, Loss: 4.0508\n",
            " Q&A Epoch 6/50, Batch 35, Loss: 4.7940\n",
            " Q&A Epoch 6/50, Batch 40, Loss: 4.2758\n",
            " Q&A Epoch 6/50, Batch 45, Loss: 4.8832\n",
            " Q&A Epoch 6/50, Batch 50, Loss: 4.6553\n",
            " Q&A Epoch 6/50, Batch 55, Loss: 4.5935\n",
            " Q&A Epoch 6/50, Batch 60, Loss: 3.9789\n",
            " Q&A Epoch 6/50, Batch 65, Loss: 4.0695\n",
            "✅ Q&A Epoch 6 completed, Average Loss: 4.2753\n",
            " Q&A Epoch 7/50, Batch 5, Loss: 4.5599\n",
            " Q&A Epoch 7/50, Batch 10, Loss: 3.9430\n",
            " Q&A Epoch 7/50, Batch 15, Loss: 4.0434\n",
            " Q&A Epoch 7/50, Batch 20, Loss: 4.0697\n",
            " Q&A Epoch 7/50, Batch 25, Loss: 4.3069\n",
            " Q&A Epoch 7/50, Batch 30, Loss: 4.0548\n",
            " Q&A Epoch 7/50, Batch 35, Loss: 4.5341\n",
            " Q&A Epoch 7/50, Batch 40, Loss: 4.1565\n",
            " Q&A Epoch 7/50, Batch 45, Loss: 4.7668\n",
            " Q&A Epoch 7/50, Batch 50, Loss: 4.4727\n",
            " Q&A Epoch 7/50, Batch 55, Loss: 4.5009\n",
            " Q&A Epoch 7/50, Batch 60, Loss: 3.6663\n",
            " Q&A Epoch 7/50, Batch 65, Loss: 3.8970\n",
            "✅ Q&A Epoch 7 completed, Average Loss: 4.1111\n",
            " Q&A Epoch 8/50, Batch 5, Loss: 4.2965\n",
            " Q&A Epoch 8/50, Batch 10, Loss: 3.8319\n",
            " Q&A Epoch 8/50, Batch 15, Loss: 3.8319\n",
            " Q&A Epoch 8/50, Batch 20, Loss: 3.8670\n",
            " Q&A Epoch 8/50, Batch 25, Loss: 4.2581\n",
            " Q&A Epoch 8/50, Batch 30, Loss: 3.7972\n",
            " Q&A Epoch 8/50, Batch 35, Loss: 4.4160\n",
            " Q&A Epoch 8/50, Batch 40, Loss: 3.9992\n",
            " Q&A Epoch 8/50, Batch 45, Loss: 4.8299\n",
            " Q&A Epoch 8/50, Batch 50, Loss: 4.3788\n",
            " Q&A Epoch 8/50, Batch 55, Loss: 4.3301\n",
            " Q&A Epoch 8/50, Batch 60, Loss: 3.6592\n",
            " Q&A Epoch 8/50, Batch 65, Loss: 3.7522\n",
            "✅ Q&A Epoch 8 completed, Average Loss: 3.9837\n",
            " Q&A Epoch 9/50, Batch 5, Loss: 4.2587\n",
            " Q&A Epoch 9/50, Batch 10, Loss: 3.6257\n",
            " Q&A Epoch 9/50, Batch 15, Loss: 3.8115\n",
            " Q&A Epoch 9/50, Batch 20, Loss: 3.7576\n",
            " Q&A Epoch 9/50, Batch 25, Loss: 4.1876\n",
            " Q&A Epoch 9/50, Batch 30, Loss: 3.7302\n",
            " Q&A Epoch 9/50, Batch 35, Loss: 4.3841\n",
            " Q&A Epoch 9/50, Batch 40, Loss: 3.9043\n",
            " Q&A Epoch 9/50, Batch 45, Loss: 4.5549\n",
            " Q&A Epoch 9/50, Batch 50, Loss: 4.2725\n",
            " Q&A Epoch 9/50, Batch 55, Loss: 4.0954\n",
            " Q&A Epoch 9/50, Batch 60, Loss: 3.4957\n",
            " Q&A Epoch 9/50, Batch 65, Loss: 3.6688\n",
            "✅ Q&A Epoch 9 completed, Average Loss: 3.8594\n",
            " Q&A Epoch 10/50, Batch 5, Loss: 4.0585\n",
            " Q&A Epoch 10/50, Batch 10, Loss: 3.5715\n",
            " Q&A Epoch 10/50, Batch 15, Loss: 3.6761\n",
            " Q&A Epoch 10/50, Batch 20, Loss: 3.5784\n",
            " Q&A Epoch 10/50, Batch 25, Loss: 4.0206\n",
            " Q&A Epoch 10/50, Batch 30, Loss: 3.5166\n",
            " Q&A Epoch 10/50, Batch 35, Loss: 4.2441\n",
            " Q&A Epoch 10/50, Batch 40, Loss: 3.7899\n",
            " Q&A Epoch 10/50, Batch 45, Loss: 4.3954\n",
            " Q&A Epoch 10/50, Batch 50, Loss: 4.0413\n",
            " Q&A Epoch 10/50, Batch 55, Loss: 4.0766\n",
            " Q&A Epoch 10/50, Batch 60, Loss: 3.3976\n",
            " Q&A Epoch 10/50, Batch 65, Loss: 3.4165\n",
            "✅ Q&A Epoch 10 completed, Average Loss: 3.7214\n",
            " Q&A Epoch 11/50, Batch 5, Loss: 3.9356\n",
            " Q&A Epoch 11/50, Batch 10, Loss: 3.4973\n",
            " Q&A Epoch 11/50, Batch 15, Loss: 3.6306\n",
            " Q&A Epoch 11/50, Batch 20, Loss: 3.5169\n",
            " Q&A Epoch 11/50, Batch 25, Loss: 3.8982\n",
            " Q&A Epoch 11/50, Batch 30, Loss: 3.4319\n",
            " Q&A Epoch 11/50, Batch 35, Loss: 4.1865\n",
            " Q&A Epoch 11/50, Batch 40, Loss: 3.5724\n",
            " Q&A Epoch 11/50, Batch 45, Loss: 4.4454\n",
            " Q&A Epoch 11/50, Batch 50, Loss: 3.9632\n",
            " Q&A Epoch 11/50, Batch 55, Loss: 3.9269\n",
            " Q&A Epoch 11/50, Batch 60, Loss: 3.2413\n",
            " Q&A Epoch 11/50, Batch 65, Loss: 3.2999\n",
            "✅ Q&A Epoch 11 completed, Average Loss: 3.6055\n",
            " Q&A Epoch 12/50, Batch 5, Loss: 3.7950\n",
            " Q&A Epoch 12/50, Batch 10, Loss: 3.3822\n",
            " Q&A Epoch 12/50, Batch 15, Loss: 3.3404\n",
            " Q&A Epoch 12/50, Batch 20, Loss: 3.4403\n",
            " Q&A Epoch 12/50, Batch 25, Loss: 3.7924\n",
            " Q&A Epoch 12/50, Batch 30, Loss: 3.4335\n",
            " Q&A Epoch 12/50, Batch 35, Loss: 4.0280\n",
            " Q&A Epoch 12/50, Batch 40, Loss: 3.4477\n",
            " Q&A Epoch 12/50, Batch 45, Loss: 4.1733\n",
            " Q&A Epoch 12/50, Batch 50, Loss: 3.8449\n",
            " Q&A Epoch 12/50, Batch 55, Loss: 3.8875\n",
            " Q&A Epoch 12/50, Batch 60, Loss: 3.1699\n",
            " Q&A Epoch 12/50, Batch 65, Loss: 3.1394\n",
            "✅ Q&A Epoch 12 completed, Average Loss: 3.4804\n",
            " Q&A Epoch 13/50, Batch 5, Loss: 3.6487\n",
            " Q&A Epoch 13/50, Batch 10, Loss: 3.4088\n",
            " Q&A Epoch 13/50, Batch 15, Loss: 3.2890\n",
            " Q&A Epoch 13/50, Batch 20, Loss: 3.1069\n",
            " Q&A Epoch 13/50, Batch 25, Loss: 3.6109\n",
            " Q&A Epoch 13/50, Batch 30, Loss: 3.2432\n",
            " Q&A Epoch 13/50, Batch 35, Loss: 3.7026\n",
            " Q&A Epoch 13/50, Batch 40, Loss: 3.2512\n",
            " Q&A Epoch 13/50, Batch 45, Loss: 4.0292\n",
            " Q&A Epoch 13/50, Batch 50, Loss: 3.7273\n",
            " Q&A Epoch 13/50, Batch 55, Loss: 3.7618\n",
            " Q&A Epoch 13/50, Batch 60, Loss: 3.0050\n",
            " Q&A Epoch 13/50, Batch 65, Loss: 2.9679\n",
            "✅ Q&A Epoch 13 completed, Average Loss: 3.3357\n",
            " Q&A Epoch 14/50, Batch 5, Loss: 3.6281\n",
            " Q&A Epoch 14/50, Batch 10, Loss: 3.2953\n",
            " Q&A Epoch 14/50, Batch 15, Loss: 3.2582\n",
            " Q&A Epoch 14/50, Batch 20, Loss: 2.9363\n",
            " Q&A Epoch 14/50, Batch 25, Loss: 3.5312\n",
            " Q&A Epoch 14/50, Batch 30, Loss: 3.0859\n",
            " Q&A Epoch 14/50, Batch 35, Loss: 3.5701\n",
            " Q&A Epoch 14/50, Batch 40, Loss: 3.0421\n",
            " Q&A Epoch 14/50, Batch 45, Loss: 3.8259\n",
            " Q&A Epoch 14/50, Batch 50, Loss: 3.3736\n",
            " Q&A Epoch 14/50, Batch 55, Loss: 3.5821\n",
            " Q&A Epoch 14/50, Batch 60, Loss: 2.9775\n",
            " Q&A Epoch 14/50, Batch 65, Loss: 2.9014\n",
            "✅ Q&A Epoch 14 completed, Average Loss: 3.2028\n",
            " Q&A Epoch 15/50, Batch 5, Loss: 3.3081\n",
            " Q&A Epoch 15/50, Batch 10, Loss: 3.1890\n",
            " Q&A Epoch 15/50, Batch 15, Loss: 2.9997\n",
            " Q&A Epoch 15/50, Batch 20, Loss: 2.9346\n",
            " Q&A Epoch 15/50, Batch 25, Loss: 3.4599\n",
            " Q&A Epoch 15/50, Batch 30, Loss: 2.9676\n",
            " Q&A Epoch 15/50, Batch 35, Loss: 3.4470\n",
            " Q&A Epoch 15/50, Batch 40, Loss: 2.9984\n",
            " Q&A Epoch 15/50, Batch 45, Loss: 3.6322\n",
            " Q&A Epoch 15/50, Batch 50, Loss: 3.3037\n",
            " Q&A Epoch 15/50, Batch 55, Loss: 3.3644\n",
            " Q&A Epoch 15/50, Batch 60, Loss: 2.7913\n",
            " Q&A Epoch 15/50, Batch 65, Loss: 2.7704\n",
            "✅ Q&A Epoch 15 completed, Average Loss: 3.0786\n",
            " Q&A Epoch 16/50, Batch 5, Loss: 3.4205\n",
            " Q&A Epoch 16/50, Batch 10, Loss: 3.0523\n",
            " Q&A Epoch 16/50, Batch 15, Loss: 2.8751\n",
            " Q&A Epoch 16/50, Batch 20, Loss: 2.9285\n",
            " Q&A Epoch 16/50, Batch 25, Loss: 3.3882\n",
            " Q&A Epoch 16/50, Batch 30, Loss: 2.8489\n",
            " Q&A Epoch 16/50, Batch 35, Loss: 3.2169\n",
            " Q&A Epoch 16/50, Batch 40, Loss: 2.8510\n",
            " Q&A Epoch 16/50, Batch 45, Loss: 3.5743\n",
            " Q&A Epoch 16/50, Batch 50, Loss: 3.1680\n",
            " Q&A Epoch 16/50, Batch 55, Loss: 3.2719\n",
            " Q&A Epoch 16/50, Batch 60, Loss: 2.8052\n",
            " Q&A Epoch 16/50, Batch 65, Loss: 2.5948\n",
            "✅ Q&A Epoch 16 completed, Average Loss: 2.9618\n",
            " Q&A Epoch 17/50, Batch 5, Loss: 3.0717\n",
            " Q&A Epoch 17/50, Batch 10, Loss: 2.9730\n",
            " Q&A Epoch 17/50, Batch 15, Loss: 2.8765\n",
            " Q&A Epoch 17/50, Batch 20, Loss: 2.7634\n",
            " Q&A Epoch 17/50, Batch 25, Loss: 3.0847\n",
            " Q&A Epoch 17/50, Batch 30, Loss: 2.8001\n",
            " Q&A Epoch 17/50, Batch 35, Loss: 3.2442\n",
            " Q&A Epoch 17/50, Batch 40, Loss: 2.7683\n",
            " Q&A Epoch 17/50, Batch 45, Loss: 3.4110\n",
            " Q&A Epoch 17/50, Batch 50, Loss: 3.0041\n",
            " Q&A Epoch 17/50, Batch 55, Loss: 2.9814\n",
            " Q&A Epoch 17/50, Batch 60, Loss: 2.6034\n",
            " Q&A Epoch 17/50, Batch 65, Loss: 2.4925\n",
            "✅ Q&A Epoch 17 completed, Average Loss: 2.8315\n",
            " Q&A Epoch 18/50, Batch 5, Loss: 2.8953\n",
            " Q&A Epoch 18/50, Batch 10, Loss: 2.8581\n",
            " Q&A Epoch 18/50, Batch 15, Loss: 2.8190\n",
            " Q&A Epoch 18/50, Batch 20, Loss: 2.6428\n",
            " Q&A Epoch 18/50, Batch 25, Loss: 3.1629\n",
            " Q&A Epoch 18/50, Batch 30, Loss: 2.6380\n",
            " Q&A Epoch 18/50, Batch 35, Loss: 3.1088\n",
            " Q&A Epoch 18/50, Batch 40, Loss: 2.5844\n",
            " Q&A Epoch 18/50, Batch 45, Loss: 3.3569\n",
            " Q&A Epoch 18/50, Batch 50, Loss: 2.9089\n",
            " Q&A Epoch 18/50, Batch 55, Loss: 2.9760\n",
            " Q&A Epoch 18/50, Batch 60, Loss: 2.3560\n",
            " Q&A Epoch 18/50, Batch 65, Loss: 2.5424\n",
            "✅ Q&A Epoch 18 completed, Average Loss: 2.7269\n",
            " Q&A Epoch 19/50, Batch 5, Loss: 2.9259\n",
            " Q&A Epoch 19/50, Batch 10, Loss: 2.7646\n",
            " Q&A Epoch 19/50, Batch 15, Loss: 2.6733\n",
            " Q&A Epoch 19/50, Batch 20, Loss: 2.6144\n",
            " Q&A Epoch 19/50, Batch 25, Loss: 2.9718\n",
            " Q&A Epoch 19/50, Batch 30, Loss: 2.5737\n",
            " Q&A Epoch 19/50, Batch 35, Loss: 2.9516\n",
            " Q&A Epoch 19/50, Batch 40, Loss: 2.4372\n",
            " Q&A Epoch 19/50, Batch 45, Loss: 3.1698\n",
            " Q&A Epoch 19/50, Batch 50, Loss: 2.7806\n",
            " Q&A Epoch 19/50, Batch 55, Loss: 2.9036\n",
            " Q&A Epoch 19/50, Batch 60, Loss: 2.3153\n",
            " Q&A Epoch 19/50, Batch 65, Loss: 2.2638\n",
            "✅ Q&A Epoch 19 completed, Average Loss: 2.6161\n",
            " Q&A Epoch 20/50, Batch 5, Loss: 2.6665\n",
            " Q&A Epoch 20/50, Batch 10, Loss: 2.7691\n",
            " Q&A Epoch 20/50, Batch 15, Loss: 2.6150\n",
            " Q&A Epoch 20/50, Batch 20, Loss: 2.4578\n",
            " Q&A Epoch 20/50, Batch 25, Loss: 2.9829\n",
            " Q&A Epoch 20/50, Batch 30, Loss: 2.4212\n",
            " Q&A Epoch 20/50, Batch 35, Loss: 2.7385\n",
            " Q&A Epoch 20/50, Batch 40, Loss: 2.3498\n",
            " Q&A Epoch 20/50, Batch 45, Loss: 2.9150\n",
            " Q&A Epoch 20/50, Batch 50, Loss: 2.7824\n",
            " Q&A Epoch 20/50, Batch 55, Loss: 2.7097\n",
            " Q&A Epoch 20/50, Batch 60, Loss: 2.1385\n",
            " Q&A Epoch 20/50, Batch 65, Loss: 2.2418\n",
            "✅ Q&A Epoch 20 completed, Average Loss: 2.4983\n",
            " Q&A Epoch 21/50, Batch 5, Loss: 2.5007\n",
            " Q&A Epoch 21/50, Batch 10, Loss: 2.4966\n",
            " Q&A Epoch 21/50, Batch 15, Loss: 2.5376\n",
            " Q&A Epoch 21/50, Batch 20, Loss: 2.3990\n",
            " Q&A Epoch 21/50, Batch 25, Loss: 2.7887\n",
            " Q&A Epoch 21/50, Batch 30, Loss: 2.4268\n",
            " Q&A Epoch 21/50, Batch 35, Loss: 2.5527\n",
            " Q&A Epoch 21/50, Batch 40, Loss: 2.2923\n",
            " Q&A Epoch 21/50, Batch 45, Loss: 2.9258\n",
            " Q&A Epoch 21/50, Batch 50, Loss: 2.7503\n",
            " Q&A Epoch 21/50, Batch 55, Loss: 2.6305\n",
            " Q&A Epoch 21/50, Batch 60, Loss: 2.2155\n",
            " Q&A Epoch 21/50, Batch 65, Loss: 2.2069\n",
            "✅ Q&A Epoch 21 completed, Average Loss: 2.4013\n",
            " Q&A Epoch 22/50, Batch 5, Loss: 2.2630\n",
            " Q&A Epoch 22/50, Batch 10, Loss: 2.3793\n",
            " Q&A Epoch 22/50, Batch 15, Loss: 2.4801\n",
            " Q&A Epoch 22/50, Batch 20, Loss: 2.2537\n",
            " Q&A Epoch 22/50, Batch 25, Loss: 2.7985\n",
            " Q&A Epoch 22/50, Batch 30, Loss: 2.3053\n",
            " Q&A Epoch 22/50, Batch 35, Loss: 2.6115\n",
            " Q&A Epoch 22/50, Batch 40, Loss: 2.2422\n",
            " Q&A Epoch 22/50, Batch 45, Loss: 2.7375\n",
            " Q&A Epoch 22/50, Batch 50, Loss: 2.6187\n",
            " Q&A Epoch 22/50, Batch 55, Loss: 2.3583\n",
            " Q&A Epoch 22/50, Batch 60, Loss: 1.9846\n",
            " Q&A Epoch 22/50, Batch 65, Loss: 2.0196\n",
            "✅ Q&A Epoch 22 completed, Average Loss: 2.2960\n",
            " Q&A Epoch 23/50, Batch 5, Loss: 2.2639\n",
            " Q&A Epoch 23/50, Batch 10, Loss: 2.2887\n",
            " Q&A Epoch 23/50, Batch 15, Loss: 2.2988\n",
            " Q&A Epoch 23/50, Batch 20, Loss: 1.9881\n",
            " Q&A Epoch 23/50, Batch 25, Loss: 2.6995\n",
            " Q&A Epoch 23/50, Batch 30, Loss: 2.2367\n",
            " Q&A Epoch 23/50, Batch 35, Loss: 2.4935\n",
            " Q&A Epoch 23/50, Batch 40, Loss: 2.1249\n",
            " Q&A Epoch 23/50, Batch 45, Loss: 2.7468\n",
            " Q&A Epoch 23/50, Batch 50, Loss: 2.5134\n",
            " Q&A Epoch 23/50, Batch 55, Loss: 2.4423\n",
            " Q&A Epoch 23/50, Batch 60, Loss: 1.8617\n",
            " Q&A Epoch 23/50, Batch 65, Loss: 1.9844\n",
            "✅ Q&A Epoch 23 completed, Average Loss: 2.1789\n",
            " Q&A Epoch 24/50, Batch 5, Loss: 2.1182\n",
            " Q&A Epoch 24/50, Batch 10, Loss: 2.2888\n",
            " Q&A Epoch 24/50, Batch 15, Loss: 2.2455\n",
            " Q&A Epoch 24/50, Batch 20, Loss: 1.9840\n",
            " Q&A Epoch 24/50, Batch 25, Loss: 2.6095\n",
            " Q&A Epoch 24/50, Batch 30, Loss: 2.1010\n",
            " Q&A Epoch 24/50, Batch 35, Loss: 2.3372\n",
            " Q&A Epoch 24/50, Batch 40, Loss: 2.0723\n",
            " Q&A Epoch 24/50, Batch 45, Loss: 2.4961\n",
            " Q&A Epoch 24/50, Batch 50, Loss: 2.3255\n",
            " Q&A Epoch 24/50, Batch 55, Loss: 2.4789\n",
            " Q&A Epoch 24/50, Batch 60, Loss: 1.7929\n",
            " Q&A Epoch 24/50, Batch 65, Loss: 1.7751\n",
            "✅ Q&A Epoch 24 completed, Average Loss: 2.0943\n",
            " Q&A Epoch 25/50, Batch 5, Loss: 1.9081\n",
            " Q&A Epoch 25/50, Batch 10, Loss: 2.1105\n",
            " Q&A Epoch 25/50, Batch 15, Loss: 2.1949\n",
            " Q&A Epoch 25/50, Batch 20, Loss: 1.8683\n",
            " Q&A Epoch 25/50, Batch 25, Loss: 2.3963\n",
            " Q&A Epoch 25/50, Batch 30, Loss: 2.0015\n",
            " Q&A Epoch 25/50, Batch 35, Loss: 2.3664\n",
            " Q&A Epoch 25/50, Batch 40, Loss: 1.9204\n",
            " Q&A Epoch 25/50, Batch 45, Loss: 2.3642\n",
            " Q&A Epoch 25/50, Batch 50, Loss: 2.2002\n",
            " Q&A Epoch 25/50, Batch 55, Loss: 2.2233\n",
            " Q&A Epoch 25/50, Batch 60, Loss: 1.6329\n",
            " Q&A Epoch 25/50, Batch 65, Loss: 1.9796\n",
            "✅ Q&A Epoch 25 completed, Average Loss: 1.9968\n",
            " Q&A Epoch 26/50, Batch 5, Loss: 2.0013\n",
            " Q&A Epoch 26/50, Batch 10, Loss: 2.0680\n",
            " Q&A Epoch 26/50, Batch 15, Loss: 1.9408\n",
            " Q&A Epoch 26/50, Batch 20, Loss: 1.7964\n",
            " Q&A Epoch 26/50, Batch 25, Loss: 2.4121\n",
            " Q&A Epoch 26/50, Batch 30, Loss: 1.9941\n",
            " Q&A Epoch 26/50, Batch 35, Loss: 2.2179\n",
            " Q&A Epoch 26/50, Batch 40, Loss: 1.6533\n",
            " Q&A Epoch 26/50, Batch 45, Loss: 2.2006\n",
            " Q&A Epoch 26/50, Batch 50, Loss: 2.1720\n",
            " Q&A Epoch 26/50, Batch 55, Loss: 2.2383\n",
            " Q&A Epoch 26/50, Batch 60, Loss: 1.7504\n",
            " Q&A Epoch 26/50, Batch 65, Loss: 1.7435\n",
            "✅ Q&A Epoch 26 completed, Average Loss: 1.9107\n",
            " Q&A Epoch 27/50, Batch 5, Loss: 1.8420\n",
            " Q&A Epoch 27/50, Batch 10, Loss: 2.1486\n",
            " Q&A Epoch 27/50, Batch 15, Loss: 1.9069\n",
            " Q&A Epoch 27/50, Batch 20, Loss: 1.7133\n",
            " Q&A Epoch 27/50, Batch 25, Loss: 2.2667\n",
            " Q&A Epoch 27/50, Batch 30, Loss: 1.9204\n",
            " Q&A Epoch 27/50, Batch 35, Loss: 2.1784\n",
            " Q&A Epoch 27/50, Batch 40, Loss: 1.6925\n",
            " Q&A Epoch 27/50, Batch 45, Loss: 2.0838\n",
            " Q&A Epoch 27/50, Batch 50, Loss: 2.2137\n",
            " Q&A Epoch 27/50, Batch 55, Loss: 2.1372\n",
            " Q&A Epoch 27/50, Batch 60, Loss: 1.6714\n",
            " Q&A Epoch 27/50, Batch 65, Loss: 1.7633\n",
            "✅ Q&A Epoch 27 completed, Average Loss: 1.8374\n",
            " Q&A Epoch 28/50, Batch 5, Loss: 1.8922\n",
            " Q&A Epoch 28/50, Batch 10, Loss: 1.8970\n",
            " Q&A Epoch 28/50, Batch 15, Loss: 1.8782\n",
            " Q&A Epoch 28/50, Batch 20, Loss: 1.7038\n",
            " Q&A Epoch 28/50, Batch 25, Loss: 2.2293\n",
            " Q&A Epoch 28/50, Batch 30, Loss: 1.7568\n",
            " Q&A Epoch 28/50, Batch 35, Loss: 2.1026\n",
            " Q&A Epoch 28/50, Batch 40, Loss: 1.6806\n",
            " Q&A Epoch 28/50, Batch 45, Loss: 2.0560\n",
            " Q&A Epoch 28/50, Batch 50, Loss: 2.0299\n",
            " Q&A Epoch 28/50, Batch 55, Loss: 2.0334\n",
            " Q&A Epoch 28/50, Batch 60, Loss: 1.6426\n",
            " Q&A Epoch 28/50, Batch 65, Loss: 1.6249\n",
            "✅ Q&A Epoch 28 completed, Average Loss: 1.7419\n",
            " Q&A Epoch 29/50, Batch 5, Loss: 1.6779\n",
            " Q&A Epoch 29/50, Batch 10, Loss: 1.9647\n",
            " Q&A Epoch 29/50, Batch 15, Loss: 1.7165\n",
            " Q&A Epoch 29/50, Batch 20, Loss: 1.3876\n",
            " Q&A Epoch 29/50, Batch 25, Loss: 2.0712\n",
            " Q&A Epoch 29/50, Batch 30, Loss: 1.7520\n",
            " Q&A Epoch 29/50, Batch 35, Loss: 1.8141\n",
            " Q&A Epoch 29/50, Batch 40, Loss: 1.5299\n",
            " Q&A Epoch 29/50, Batch 45, Loss: 1.9009\n",
            " Q&A Epoch 29/50, Batch 50, Loss: 1.9333\n",
            " Q&A Epoch 29/50, Batch 55, Loss: 1.8252\n",
            " Q&A Epoch 29/50, Batch 60, Loss: 1.6217\n",
            " Q&A Epoch 29/50, Batch 65, Loss: 1.4953\n",
            "✅ Q&A Epoch 29 completed, Average Loss: 1.6436\n",
            " Q&A Epoch 30/50, Batch 5, Loss: 1.6207\n",
            " Q&A Epoch 30/50, Batch 10, Loss: 1.7623\n",
            " Q&A Epoch 30/50, Batch 15, Loss: 1.5624\n",
            " Q&A Epoch 30/50, Batch 20, Loss: 1.4003\n",
            " Q&A Epoch 30/50, Batch 25, Loss: 2.1450\n",
            " Q&A Epoch 30/50, Batch 30, Loss: 1.6210\n",
            " Q&A Epoch 30/50, Batch 35, Loss: 1.8129\n",
            " Q&A Epoch 30/50, Batch 40, Loss: 1.3518\n",
            " Q&A Epoch 30/50, Batch 45, Loss: 1.9466\n",
            " Q&A Epoch 30/50, Batch 50, Loss: 1.8297\n",
            " Q&A Epoch 30/50, Batch 55, Loss: 1.8599\n",
            " Q&A Epoch 30/50, Batch 60, Loss: 1.3766\n",
            " Q&A Epoch 30/50, Batch 65, Loss: 1.4407\n",
            "✅ Q&A Epoch 30 completed, Average Loss: 1.5803\n",
            " Q&A Epoch 31/50, Batch 5, Loss: 1.5292\n",
            " Q&A Epoch 31/50, Batch 10, Loss: 1.7057\n",
            " Q&A Epoch 31/50, Batch 15, Loss: 1.5079\n",
            " Q&A Epoch 31/50, Batch 20, Loss: 1.3733\n",
            " Q&A Epoch 31/50, Batch 25, Loss: 1.9128\n",
            " Q&A Epoch 31/50, Batch 30, Loss: 1.5720\n",
            " Q&A Epoch 31/50, Batch 35, Loss: 1.7230\n",
            " Q&A Epoch 31/50, Batch 40, Loss: 1.4575\n",
            " Q&A Epoch 31/50, Batch 45, Loss: 1.6510\n",
            " Q&A Epoch 31/50, Batch 50, Loss: 1.8738\n",
            " Q&A Epoch 31/50, Batch 55, Loss: 1.7676\n",
            " Q&A Epoch 31/50, Batch 60, Loss: 1.3468\n",
            " Q&A Epoch 31/50, Batch 65, Loss: 1.3796\n",
            "✅ Q&A Epoch 31 completed, Average Loss: 1.5139\n",
            " Q&A Epoch 32/50, Batch 5, Loss: 1.3391\n",
            " Q&A Epoch 32/50, Batch 10, Loss: 1.7877\n",
            " Q&A Epoch 32/50, Batch 15, Loss: 1.4850\n",
            " Q&A Epoch 32/50, Batch 20, Loss: 1.3878\n",
            " Q&A Epoch 32/50, Batch 25, Loss: 1.8618\n",
            " Q&A Epoch 32/50, Batch 30, Loss: 1.4918\n",
            " Q&A Epoch 32/50, Batch 35, Loss: 1.7167\n",
            " Q&A Epoch 32/50, Batch 40, Loss: 1.3047\n",
            " Q&A Epoch 32/50, Batch 45, Loss: 1.7498\n",
            " Q&A Epoch 32/50, Batch 50, Loss: 1.8198\n",
            " Q&A Epoch 32/50, Batch 55, Loss: 1.6765\n",
            " Q&A Epoch 32/50, Batch 60, Loss: 1.3670\n",
            " Q&A Epoch 32/50, Batch 65, Loss: 1.2142\n",
            "✅ Q&A Epoch 32 completed, Average Loss: 1.4530\n",
            " Q&A Epoch 33/50, Batch 5, Loss: 1.3720\n",
            " Q&A Epoch 33/50, Batch 10, Loss: 1.6356\n",
            " Q&A Epoch 33/50, Batch 15, Loss: 1.3907\n",
            " Q&A Epoch 33/50, Batch 20, Loss: 1.4333\n",
            " Q&A Epoch 33/50, Batch 25, Loss: 1.8804\n",
            " Q&A Epoch 33/50, Batch 30, Loss: 1.4913\n",
            " Q&A Epoch 33/50, Batch 35, Loss: 1.5369\n",
            " Q&A Epoch 33/50, Batch 40, Loss: 1.2464\n",
            " Q&A Epoch 33/50, Batch 45, Loss: 1.6458\n",
            " Q&A Epoch 33/50, Batch 50, Loss: 1.5787\n",
            " Q&A Epoch 33/50, Batch 55, Loss: 1.7787\n",
            " Q&A Epoch 33/50, Batch 60, Loss: 1.3345\n",
            " Q&A Epoch 33/50, Batch 65, Loss: 1.1935\n",
            "✅ Q&A Epoch 33 completed, Average Loss: 1.3881\n",
            " Q&A Epoch 34/50, Batch 5, Loss: 1.2171\n",
            " Q&A Epoch 34/50, Batch 10, Loss: 1.4386\n",
            " Q&A Epoch 34/50, Batch 15, Loss: 1.4178\n",
            " Q&A Epoch 34/50, Batch 20, Loss: 1.2206\n",
            " Q&A Epoch 34/50, Batch 25, Loss: 1.8370\n",
            " Q&A Epoch 34/50, Batch 30, Loss: 1.2797\n",
            " Q&A Epoch 34/50, Batch 35, Loss: 1.6061\n",
            " Q&A Epoch 34/50, Batch 40, Loss: 1.2955\n",
            " Q&A Epoch 34/50, Batch 45, Loss: 1.5554\n",
            " Q&A Epoch 34/50, Batch 50, Loss: 1.5714\n",
            " Q&A Epoch 34/50, Batch 55, Loss: 1.4937\n",
            " Q&A Epoch 34/50, Batch 60, Loss: 1.2336\n",
            " Q&A Epoch 34/50, Batch 65, Loss: 1.0943\n",
            "✅ Q&A Epoch 34 completed, Average Loss: 1.3152\n",
            " Q&A Epoch 35/50, Batch 5, Loss: 1.1234\n",
            " Q&A Epoch 35/50, Batch 10, Loss: 1.3806\n",
            " Q&A Epoch 35/50, Batch 15, Loss: 1.4001\n",
            " Q&A Epoch 35/50, Batch 20, Loss: 1.0769\n",
            " Q&A Epoch 35/50, Batch 25, Loss: 1.6127\n",
            " Q&A Epoch 35/50, Batch 30, Loss: 1.3679\n",
            " Q&A Epoch 35/50, Batch 35, Loss: 1.6042\n",
            " Q&A Epoch 35/50, Batch 40, Loss: 1.1861\n",
            " Q&A Epoch 35/50, Batch 45, Loss: 1.3171\n",
            " Q&A Epoch 35/50, Batch 50, Loss: 1.4716\n",
            " Q&A Epoch 35/50, Batch 55, Loss: 1.4669\n",
            " Q&A Epoch 35/50, Batch 60, Loss: 1.2134\n",
            " Q&A Epoch 35/50, Batch 65, Loss: 1.2348\n",
            "✅ Q&A Epoch 35 completed, Average Loss: 1.2596\n",
            " Q&A Epoch 36/50, Batch 5, Loss: 1.1383\n",
            " Q&A Epoch 36/50, Batch 10, Loss: 1.3493\n",
            " Q&A Epoch 36/50, Batch 15, Loss: 1.4298\n",
            " Q&A Epoch 36/50, Batch 20, Loss: 1.1759\n",
            " Q&A Epoch 36/50, Batch 25, Loss: 1.5547\n",
            " Q&A Epoch 36/50, Batch 30, Loss: 1.3480\n",
            " Q&A Epoch 36/50, Batch 35, Loss: 1.4211\n",
            " Q&A Epoch 36/50, Batch 40, Loss: 1.1103\n",
            " Q&A Epoch 36/50, Batch 45, Loss: 1.4222\n",
            " Q&A Epoch 36/50, Batch 50, Loss: 1.5672\n",
            " Q&A Epoch 36/50, Batch 55, Loss: 1.4490\n",
            " Q&A Epoch 36/50, Batch 60, Loss: 1.2021\n",
            " Q&A Epoch 36/50, Batch 65, Loss: 1.0029\n",
            "✅ Q&A Epoch 36 completed, Average Loss: 1.2079\n",
            " Q&A Epoch 37/50, Batch 5, Loss: 1.1139\n",
            " Q&A Epoch 37/50, Batch 10, Loss: 1.3910\n",
            " Q&A Epoch 37/50, Batch 15, Loss: 1.2300\n",
            " Q&A Epoch 37/50, Batch 20, Loss: 1.1401\n",
            " Q&A Epoch 37/50, Batch 25, Loss: 1.4773\n",
            " Q&A Epoch 37/50, Batch 30, Loss: 1.2869\n",
            " Q&A Epoch 37/50, Batch 35, Loss: 1.4977\n",
            " Q&A Epoch 37/50, Batch 40, Loss: 1.0805\n",
            " Q&A Epoch 37/50, Batch 45, Loss: 1.3239\n",
            " Q&A Epoch 37/50, Batch 50, Loss: 1.3657\n",
            " Q&A Epoch 37/50, Batch 55, Loss: 1.4273\n",
            " Q&A Epoch 37/50, Batch 60, Loss: 1.1129\n",
            " Q&A Epoch 37/50, Batch 65, Loss: 1.0564\n",
            "✅ Q&A Epoch 37 completed, Average Loss: 1.1629\n",
            " Q&A Epoch 38/50, Batch 5, Loss: 1.0828\n",
            " Q&A Epoch 38/50, Batch 10, Loss: 1.4187\n",
            " Q&A Epoch 38/50, Batch 15, Loss: 1.2692\n",
            " Q&A Epoch 38/50, Batch 20, Loss: 1.1636\n",
            " Q&A Epoch 38/50, Batch 25, Loss: 1.3675\n",
            " Q&A Epoch 38/50, Batch 30, Loss: 1.1889\n",
            " Q&A Epoch 38/50, Batch 35, Loss: 1.1862\n",
            " Q&A Epoch 38/50, Batch 40, Loss: 0.9447\n",
            " Q&A Epoch 38/50, Batch 45, Loss: 1.3363\n",
            " Q&A Epoch 38/50, Batch 50, Loss: 1.3702\n",
            " Q&A Epoch 38/50, Batch 55, Loss: 1.1891\n",
            " Q&A Epoch 38/50, Batch 60, Loss: 1.2311\n",
            " Q&A Epoch 38/50, Batch 65, Loss: 1.0590\n",
            "✅ Q&A Epoch 38 completed, Average Loss: 1.1045\n",
            " Q&A Epoch 39/50, Batch 5, Loss: 0.9774\n",
            " Q&A Epoch 39/50, Batch 10, Loss: 1.3532\n",
            " Q&A Epoch 39/50, Batch 15, Loss: 1.0943\n",
            " Q&A Epoch 39/50, Batch 20, Loss: 1.0270\n",
            " Q&A Epoch 39/50, Batch 25, Loss: 1.2822\n",
            " Q&A Epoch 39/50, Batch 30, Loss: 1.1594\n",
            " Q&A Epoch 39/50, Batch 35, Loss: 1.1903\n",
            " Q&A Epoch 39/50, Batch 40, Loss: 0.8773\n",
            " Q&A Epoch 39/50, Batch 45, Loss: 1.1522\n",
            " Q&A Epoch 39/50, Batch 50, Loss: 1.4670\n",
            " Q&A Epoch 39/50, Batch 55, Loss: 1.2852\n",
            " Q&A Epoch 39/50, Batch 60, Loss: 1.0427\n",
            " Q&A Epoch 39/50, Batch 65, Loss: 0.8798\n",
            "✅ Q&A Epoch 39 completed, Average Loss: 1.0456\n",
            " Q&A Epoch 40/50, Batch 5, Loss: 0.9414\n",
            " Q&A Epoch 40/50, Batch 10, Loss: 1.2026\n",
            " Q&A Epoch 40/50, Batch 15, Loss: 1.1645\n",
            " Q&A Epoch 40/50, Batch 20, Loss: 1.0262\n",
            " Q&A Epoch 40/50, Batch 25, Loss: 1.4006\n",
            " Q&A Epoch 40/50, Batch 30, Loss: 1.0722\n",
            " Q&A Epoch 40/50, Batch 35, Loss: 1.1897\n",
            " Q&A Epoch 40/50, Batch 40, Loss: 0.8281\n",
            " Q&A Epoch 40/50, Batch 45, Loss: 1.1010\n",
            " Q&A Epoch 40/50, Batch 50, Loss: 1.2656\n",
            " Q&A Epoch 40/50, Batch 55, Loss: 1.1776\n",
            " Q&A Epoch 40/50, Batch 60, Loss: 1.0350\n",
            " Q&A Epoch 40/50, Batch 65, Loss: 0.8898\n",
            "✅ Q&A Epoch 40 completed, Average Loss: 1.0095\n",
            " Q&A Epoch 41/50, Batch 5, Loss: 0.8668\n",
            " Q&A Epoch 41/50, Batch 10, Loss: 1.2158\n",
            " Q&A Epoch 41/50, Batch 15, Loss: 1.1104\n",
            " Q&A Epoch 41/50, Batch 20, Loss: 0.9553\n",
            " Q&A Epoch 41/50, Batch 25, Loss: 1.2205\n",
            " Q&A Epoch 41/50, Batch 30, Loss: 1.0980\n",
            " Q&A Epoch 41/50, Batch 35, Loss: 1.0813\n",
            " Q&A Epoch 41/50, Batch 40, Loss: 0.9133\n",
            " Q&A Epoch 41/50, Batch 45, Loss: 0.9312\n",
            " Q&A Epoch 41/50, Batch 50, Loss: 1.2652\n",
            " Q&A Epoch 41/50, Batch 55, Loss: 1.2645\n",
            " Q&A Epoch 41/50, Batch 60, Loss: 0.9369\n",
            " Q&A Epoch 41/50, Batch 65, Loss: 0.7735\n",
            "✅ Q&A Epoch 41 completed, Average Loss: 0.9630\n",
            " Q&A Epoch 42/50, Batch 5, Loss: 0.9253\n",
            " Q&A Epoch 42/50, Batch 10, Loss: 1.0959\n",
            " Q&A Epoch 42/50, Batch 15, Loss: 1.2162\n",
            " Q&A Epoch 42/50, Batch 20, Loss: 0.8650\n",
            " Q&A Epoch 42/50, Batch 25, Loss: 1.1809\n",
            " Q&A Epoch 42/50, Batch 30, Loss: 0.9883\n",
            " Q&A Epoch 42/50, Batch 35, Loss: 1.0522\n",
            " Q&A Epoch 42/50, Batch 40, Loss: 0.7483\n",
            " Q&A Epoch 42/50, Batch 45, Loss: 0.8962\n",
            " Q&A Epoch 42/50, Batch 50, Loss: 1.0875\n",
            " Q&A Epoch 42/50, Batch 55, Loss: 1.2648\n",
            " Q&A Epoch 42/50, Batch 60, Loss: 0.9104\n",
            " Q&A Epoch 42/50, Batch 65, Loss: 0.8015\n",
            "✅ Q&A Epoch 42 completed, Average Loss: 0.9132\n",
            " Q&A Epoch 43/50, Batch 5, Loss: 1.0105\n",
            " Q&A Epoch 43/50, Batch 10, Loss: 1.0051\n",
            " Q&A Epoch 43/50, Batch 15, Loss: 1.0416\n",
            " Q&A Epoch 43/50, Batch 20, Loss: 0.9641\n",
            " Q&A Epoch 43/50, Batch 25, Loss: 1.2263\n",
            " Q&A Epoch 43/50, Batch 30, Loss: 1.0067\n",
            " Q&A Epoch 43/50, Batch 35, Loss: 1.0005\n",
            " Q&A Epoch 43/50, Batch 40, Loss: 0.8332\n",
            " Q&A Epoch 43/50, Batch 45, Loss: 0.9380\n",
            " Q&A Epoch 43/50, Batch 50, Loss: 1.1102\n",
            " Q&A Epoch 43/50, Batch 55, Loss: 1.1532\n",
            " Q&A Epoch 43/50, Batch 60, Loss: 0.9528\n",
            " Q&A Epoch 43/50, Batch 65, Loss: 0.6402\n",
            "✅ Q&A Epoch 43 completed, Average Loss: 0.8951\n",
            " Q&A Epoch 44/50, Batch 5, Loss: 0.8997\n",
            " Q&A Epoch 44/50, Batch 10, Loss: 0.9404\n",
            " Q&A Epoch 44/50, Batch 15, Loss: 0.9084\n",
            " Q&A Epoch 44/50, Batch 20, Loss: 0.9521\n",
            " Q&A Epoch 44/50, Batch 25, Loss: 1.0420\n",
            " Q&A Epoch 44/50, Batch 30, Loss: 1.0653\n",
            " Q&A Epoch 44/50, Batch 35, Loss: 0.9496\n",
            " Q&A Epoch 44/50, Batch 40, Loss: 0.7947\n",
            " Q&A Epoch 44/50, Batch 45, Loss: 0.8677\n",
            " Q&A Epoch 44/50, Batch 50, Loss: 1.0880\n",
            " Q&A Epoch 44/50, Batch 55, Loss: 1.1340\n",
            " Q&A Epoch 44/50, Batch 60, Loss: 0.7484\n",
            " Q&A Epoch 44/50, Batch 65, Loss: 0.6763\n",
            "✅ Q&A Epoch 44 completed, Average Loss: 0.8440\n",
            " Q&A Epoch 45/50, Batch 5, Loss: 0.9302\n",
            " Q&A Epoch 45/50, Batch 10, Loss: 0.9352\n",
            " Q&A Epoch 45/50, Batch 15, Loss: 0.9665\n",
            " Q&A Epoch 45/50, Batch 20, Loss: 0.8001\n",
            " Q&A Epoch 45/50, Batch 25, Loss: 1.1416\n",
            " Q&A Epoch 45/50, Batch 30, Loss: 0.8640\n",
            " Q&A Epoch 45/50, Batch 35, Loss: 0.9502\n",
            " Q&A Epoch 45/50, Batch 40, Loss: 0.7399\n",
            " Q&A Epoch 45/50, Batch 45, Loss: 0.8433\n",
            " Q&A Epoch 45/50, Batch 50, Loss: 0.9474\n",
            " Q&A Epoch 45/50, Batch 55, Loss: 0.9719\n",
            " Q&A Epoch 45/50, Batch 60, Loss: 0.8273\n",
            " Q&A Epoch 45/50, Batch 65, Loss: 0.7212\n",
            "✅ Q&A Epoch 45 completed, Average Loss: 0.8120\n",
            " Q&A Epoch 46/50, Batch 5, Loss: 0.7701\n",
            " Q&A Epoch 46/50, Batch 10, Loss: 0.8433\n",
            " Q&A Epoch 46/50, Batch 15, Loss: 0.9187\n",
            " Q&A Epoch 46/50, Batch 20, Loss: 0.7995\n",
            " Q&A Epoch 46/50, Batch 25, Loss: 1.0503\n",
            " Q&A Epoch 46/50, Batch 30, Loss: 0.8700\n",
            " Q&A Epoch 46/50, Batch 35, Loss: 0.8332\n",
            " Q&A Epoch 46/50, Batch 40, Loss: 0.6247\n",
            " Q&A Epoch 46/50, Batch 45, Loss: 0.7800\n",
            " Q&A Epoch 46/50, Batch 50, Loss: 0.9558\n",
            " Q&A Epoch 46/50, Batch 55, Loss: 0.9805\n",
            " Q&A Epoch 46/50, Batch 60, Loss: 0.7991\n",
            " Q&A Epoch 46/50, Batch 65, Loss: 0.7397\n",
            "✅ Q&A Epoch 46 completed, Average Loss: 0.7822\n",
            " Q&A Epoch 47/50, Batch 5, Loss: 0.7671\n",
            " Q&A Epoch 47/50, Batch 10, Loss: 0.8703\n",
            " Q&A Epoch 47/50, Batch 15, Loss: 0.9124\n",
            " Q&A Epoch 47/50, Batch 20, Loss: 0.6652\n",
            " Q&A Epoch 47/50, Batch 25, Loss: 0.8825\n",
            " Q&A Epoch 47/50, Batch 30, Loss: 0.7240\n",
            " Q&A Epoch 47/50, Batch 35, Loss: 0.8616\n",
            " Q&A Epoch 47/50, Batch 40, Loss: 0.5526\n",
            " Q&A Epoch 47/50, Batch 45, Loss: 0.6893\n",
            " Q&A Epoch 47/50, Batch 50, Loss: 0.8666\n",
            " Q&A Epoch 47/50, Batch 55, Loss: 0.9605\n",
            " Q&A Epoch 47/50, Batch 60, Loss: 0.7949\n",
            " Q&A Epoch 47/50, Batch 65, Loss: 0.5528\n",
            "✅ Q&A Epoch 47 completed, Average Loss: 0.7301\n",
            " Q&A Epoch 48/50, Batch 5, Loss: 0.7224\n",
            " Q&A Epoch 48/50, Batch 10, Loss: 0.7762\n",
            " Q&A Epoch 48/50, Batch 15, Loss: 0.8404\n",
            " Q&A Epoch 48/50, Batch 20, Loss: 0.7648\n",
            " Q&A Epoch 48/50, Batch 25, Loss: 0.9320\n",
            " Q&A Epoch 48/50, Batch 30, Loss: 0.6963\n",
            " Q&A Epoch 48/50, Batch 35, Loss: 0.9023\n",
            " Q&A Epoch 48/50, Batch 40, Loss: 0.5692\n",
            " Q&A Epoch 48/50, Batch 45, Loss: 0.6367\n",
            " Q&A Epoch 48/50, Batch 50, Loss: 0.7749\n",
            " Q&A Epoch 48/50, Batch 55, Loss: 0.8818\n",
            " Q&A Epoch 48/50, Batch 60, Loss: 0.7292\n",
            " Q&A Epoch 48/50, Batch 65, Loss: 0.5041\n",
            "✅ Q&A Epoch 48 completed, Average Loss: 0.7060\n",
            " Q&A Epoch 49/50, Batch 5, Loss: 0.7107\n",
            " Q&A Epoch 49/50, Batch 10, Loss: 0.8695\n",
            " Q&A Epoch 49/50, Batch 15, Loss: 0.8565\n",
            " Q&A Epoch 49/50, Batch 20, Loss: 0.7144\n",
            " Q&A Epoch 49/50, Batch 25, Loss: 0.9752\n",
            " Q&A Epoch 49/50, Batch 30, Loss: 0.7831\n",
            " Q&A Epoch 49/50, Batch 35, Loss: 0.6568\n",
            " Q&A Epoch 49/50, Batch 40, Loss: 0.5339\n",
            " Q&A Epoch 49/50, Batch 45, Loss: 0.6573\n",
            " Q&A Epoch 49/50, Batch 50, Loss: 0.8555\n",
            " Q&A Epoch 49/50, Batch 55, Loss: 0.8895\n",
            " Q&A Epoch 49/50, Batch 60, Loss: 0.7253\n",
            " Q&A Epoch 49/50, Batch 65, Loss: 0.5837\n",
            "✅ Q&A Epoch 49 completed, Average Loss: 0.6868\n",
            " Q&A Epoch 50/50, Batch 5, Loss: 0.6592\n",
            " Q&A Epoch 50/50, Batch 10, Loss: 0.6387\n",
            " Q&A Epoch 50/50, Batch 15, Loss: 0.7689\n",
            " Q&A Epoch 50/50, Batch 20, Loss: 0.6287\n",
            " Q&A Epoch 50/50, Batch 25, Loss: 0.7682\n",
            " Q&A Epoch 50/50, Batch 30, Loss: 0.6561\n",
            " Q&A Epoch 50/50, Batch 35, Loss: 0.7217\n",
            " Q&A Epoch 50/50, Batch 40, Loss: 0.5440\n",
            " Q&A Epoch 50/50, Batch 45, Loss: 0.6448\n",
            " Q&A Epoch 50/50, Batch 50, Loss: 0.8586\n",
            " Q&A Epoch 50/50, Batch 55, Loss: 0.7458\n",
            " Q&A Epoch 50/50, Batch 60, Loss: 0.6203\n",
            " Q&A Epoch 50/50, Batch 65, Loss: 0.4835\n",
            "✅ Q&A Epoch 50 completed, Average Loss: 0.6438\n",
            "🎉 Instruction Fine-tuning completed successfully!\n",
            "💾 Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nIMPORTANT about our training loss!\")\n",
        "print(\"   - Pretraining loss should drop below 2.5\")\n",
        "print(\"   - Domain fine-tuning loss should drop below 2.0\")\n",
        "print(\"   - Q&A fine-tuning loss should drop below 1.5\")\n",
        "print(\"   If losses don't drop, we need MORE training data!\")"
      ],
      "metadata": {
        "id": "OwBdPNV4G7wZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c39139b-25d0-4cfb-c958-7bdc719c2272"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IMPORTANT about our training loss!\n",
            "   - Pretraining loss should drop below 2.5\n",
            "   - Domain fine-tuning loss should drop below 2.0\n",
            "   - Q&A fine-tuning loss should drop below 1.5\n",
            "   If losses don't drop, we need MORE training data!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"RAG system - Each query is independent, with intelligent output extraction\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, embedding_model, document_chunks):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedding_model = embedding_model\n",
        "        self.document_chunks = document_chunks\n",
        "\n",
        "        print(\"Computing embeddings for chunks...\")\n",
        "        self.chunk_embeddings = self.embedding_model.encode(document_chunks)\n",
        "        print(f\"Embeddings computed for {len(document_chunks)} chunks\")\n",
        "\n",
        "    def retrieve_relevant_chunks(self, query, top_k=3):\n",
        "        \"\"\"Retrieve relevant document chunks\"\"\"\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        similarities = cosine_similarity(query_embedding, self.chunk_embeddings)\n",
        "        top_indices = np.argsort(similarities[0])[-top_k:][::-1]\n",
        "\n",
        "        relevant_chunks = [self.document_chunks[i] for i in top_indices]\n",
        "        return relevant_chunks\n",
        "\n",
        "    def build_prompt(self, query, context_chunks):\n",
        "        \"\"\"\n",
        "        Build prompt WITHOUT conversation history (independent queries)\n",
        "        \"\"\"\n",
        "        context = \"\\n\".join([f\"- {chunk[:300]}\" for chunk in context_chunks[:2]])\n",
        "\n",
        "        #  Simple, direct prompt without history\n",
        "        prompt = f\"\"\"Based on the following context, provide a clear and accurate answer to the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def extract_answer_intelligently(self, raw_output, query):\n",
        "        \"\"\"\n",
        "        Intelligent answer extraction from model output\n",
        "        \"\"\"\n",
        "        if not raw_output or len(raw_output.strip()) == 0:\n",
        "            return None\n",
        "\n",
        "        # Remove common prefixes that models sometimes generate\n",
        "        prefixes_to_remove = [\n",
        "            \"Answer:\",\n",
        "            \"answer:\",\n",
        "            \"A:\",\n",
        "            \"Based on the context,\",\n",
        "            \"According to the context,\",\n",
        "            \"The answer is\",\n",
        "            \"the answer is\"\n",
        "        ]\n",
        "\n",
        "        cleaned = raw_output.strip()\n",
        "        for prefix in prefixes_to_remove:\n",
        "            if cleaned.startswith(prefix):\n",
        "                cleaned = cleaned[len(prefix):].strip()\n",
        "\n",
        "        # If output is just repeated tokens or gibberish, return None\n",
        "        if self.is_gibberish(cleaned):\n",
        "            return None\n",
        "\n",
        "        # Extract first complete sentence or paragraph\n",
        "        sentences = sent_tokenize(cleaned)\n",
        "\n",
        "        if not sentences:\n",
        "            return None\n",
        "\n",
        "        # Find first meaningful sentence (not too short, not gibberish)\n",
        "        meaningful_sentences = []\n",
        "        for sent in sentences:\n",
        "            if len(sent.split()) >= 5 and not self.is_gibberish(sent):\n",
        "                meaningful_sentences.append(sent)\n",
        "\n",
        "            # Stop after 3-4 good sentences for concise answers\n",
        "            if len(meaningful_sentences) >= 3:\n",
        "                break\n",
        "\n",
        "        if not meaningful_sentences:\n",
        "            return None\n",
        "\n",
        "        answer = ' '.join(meaningful_sentences)\n",
        "\n",
        "        # Final cleanup\n",
        "        answer = self.final_cleanup(answer)\n",
        "\n",
        "        return answer if len(answer.split()) >= 5 else None\n",
        "\n",
        "    def is_gibberish(self, text):\n",
        "        \"\"\"\n",
        "        Detect if text is gibberish\n",
        "        \"\"\"\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return True\n",
        "\n",
        "        # Check for excessive special characters\n",
        "        special_char_ratio = sum(1 for c in text if not c.isalnum() and not c.isspace()) / len(text)\n",
        "        if special_char_ratio > 0.3:\n",
        "            return True\n",
        "\n",
        "        # Check for repeated short patterns (like \"Al Al Al\")\n",
        "        words = text.split()\n",
        "        if len(words) > 3:\n",
        "            # Check if too many repeated words\n",
        "            unique_words = set(words)\n",
        "            if len(unique_words) / len(words) < 0.3:\n",
        "                return True\n",
        "\n",
        "        # Check for excessive single-letter words\n",
        "        single_letters = sum(1 for w in words if len(w) == 1)\n",
        "        if len(words) > 0 and single_letters / len(words) > 0.3:\n",
        "            return True\n",
        "\n",
        "        # Check for unknown characters (�)\n",
        "        if '�' in text or '\\ufffd' in text:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def final_cleanup(self, text):\n",
        "        \"\"\"\n",
        "        Final text cleanup\n",
        "        \"\"\"\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Ensure proper capitalization\n",
        "        if text and text[0].islower():\n",
        "            text = text[0].upper() + text[1:]\n",
        "\n",
        "        # Ensure proper ending punctuation\n",
        "        if text and not text.endswith(('.', '!', '?')):\n",
        "            text += '.'\n",
        "\n",
        "        return text\n",
        "\n",
        "    def generate_response(self, query, max_length=150):\n",
        "        \"\"\"\n",
        "        Generation with intelligent extraction\n",
        "        \"\"\"\n",
        "        # Retrieve context\n",
        "        relevant_chunks = self.retrieve_relevant_chunks(query, top_k=2)\n",
        "\n",
        "        # Build prompt (without history)\n",
        "        prompt = self.build_prompt(query, relevant_chunks)\n",
        "\n",
        "        # Tokenize prompt\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors='pt',\n",
        "            max_length=384,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "\n",
        "        source_ids = inputs['input_ids'].to(device)\n",
        "        source_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "        # Generate with model\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            memory = self.model.encode(source_ids, source_mask)\n",
        "\n",
        "            # Start generation\n",
        "            generated_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=device)\n",
        "\n",
        "            for step in range(max_length):\n",
        "                tgt_mask = self.model.generate_square_subsequent_mask(generated_ids.size(1)).to(device)\n",
        "\n",
        "                logits = self.model.decode(\n",
        "                    generated_ids,\n",
        "                    memory,\n",
        "                    memory_mask=~source_mask.bool(),\n",
        "                    tgt_mask=tgt_mask\n",
        "                )\n",
        "\n",
        "                next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                # Sampling parameters\n",
        "                temperature = 0.7\n",
        "                top_k = 50\n",
        "\n",
        "                next_token_logits = next_token_logits / temperature\n",
        "\n",
        "                # Top-k filtering\n",
        "                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # Repetition penalty\n",
        "                if generated_ids.size(1) > 10:\n",
        "                    repetition_penalty = 1.3\n",
        "                    recent_tokens = set(generated_ids[0, -15:].tolist())\n",
        "                    for token in recent_tokens:\n",
        "                        if next_token_logits[0, token] > 0:\n",
        "                            next_token_logits[0, token] /= repetition_penalty\n",
        "                        else:\n",
        "                            next_token_logits[0, token] *= repetition_penalty\n",
        "\n",
        "                # Sample next token\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Stop conditions\n",
        "                if next_token_id.item() in [self.tokenizer.eos_token_id, self.tokenizer.pad_token_id]:\n",
        "                    break\n",
        "\n",
        "                generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
        "\n",
        "                # Early stopping for complete answers\n",
        "                if step > 30:\n",
        "                    current_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "                    if len(current_text.split()) > 20 and any(p in current_text[-15:] for p in ['.', '!', '?']):\n",
        "                        break\n",
        "\n",
        "            # Decode response\n",
        "            raw_response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "            # Intelligent extraction\n",
        "            extracted_answer = self.extract_answer_intelligently(raw_response, query)\n",
        "\n",
        "            # Fallback to context-based answer if model output is poor\n",
        "            if extracted_answer is None or len(extracted_answer.split()) < 5:\n",
        "                extracted_answer = self.fallback_to_context(query, relevant_chunks)\n",
        "\n",
        "            return extracted_answer, relevant_chunks\n",
        "\n",
        "    def fallback_to_context(self, query, context_chunks):\n",
        "        \"\"\"\n",
        "        Fallback strategy - extract relevant sentences from context\n",
        "        \"\"\"\n",
        "        if not context_chunks:\n",
        "            return \"I don't have enough information to answer this question based on the provided context.\"\n",
        "\n",
        "        # Simple extractive approach: find most relevant sentences\n",
        "        query_words = set(query.lower().split())\n",
        "        best_sentences = []\n",
        "\n",
        "        for chunk in context_chunks[:2]:\n",
        "            sentences = sent_tokenize(chunk)\n",
        "            for sent in sentences:\n",
        "                sent_words = set(sent.lower().split())\n",
        "                overlap = len(query_words & sent_words)\n",
        "                if overlap >= 2 and len(sent.split()) >= 5:\n",
        "                    best_sentences.append((overlap, sent))\n",
        "\n",
        "        if best_sentences:\n",
        "            # Sort by overlap and take top 2-3 sentences\n",
        "            best_sentences.sort(reverse=True, key=lambda x: x[0])\n",
        "            answer = ' '.join([sent for _, sent in best_sentences[:2]])\n",
        "            return self.final_cleanup(answer)\n",
        "\n",
        "        return \"I don't have enough information to answer this question based on the provided context.\"\n",
        "\n",
        "    def chat(self, user_input):\n",
        "        \"\"\"\n",
        "        Independent query processing (no history)\n",
        "        \"\"\"\n",
        "        response, sources = self.generate_response(user_input)\n",
        "\n",
        "        return {\n",
        "            \"response\": response,\n",
        "            \"sources\": sources[:2]\n",
        "        }\n",
        "\n",
        "# Initialize the improved RAG system\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Initializing Improved RAG System (Independent Queries)...\")\n",
        "print(\"=\"*60)\n",
        "rag_system = RAGSystem(model, tokenizer, embedding_model, document_chunks)\n",
        "print(\"✅ Improved RAG System ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQGDCkf8JKlp",
        "outputId": "946c18ca-61c0-42d5-f12d-3afc1ffbb799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Initializing Improved RAG System (Independent Queries)...\n",
            "============================================================\n",
            "Computing embeddings for chunks...\n",
            "Embeddings computed for 142 chunks\n",
            "✅ Improved RAG System ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the system\n",
        "print(\"=== RAG SYSTEM TESTING ===\")\n",
        "\n",
        "test_questions = [\n",
        "    \"What is Federated Learning?\",\n",
        "    \"what is Fed_Meta Allign?\"\n",
        "]\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\nQ{i}: {question}\")\n",
        "    result = rag_system.chat(question)\n",
        "    print(f\"Answer: {result['response']}\")\n",
        "\n",
        "    # if result['sources']:\n",
        "    #     print(\"Sources used:\")\n",
        "    #     for j, source in enumerate(result['sources'], 1):\n",
        "    #         print(f\"  {j}. {source[:100]}...\")"
      ],
      "metadata": {
        "id": "NsLRU1JxVo2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "571610fa-6973-48d2-8ec0-be133381e919"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "=== CORRECTED RAG SYSTEM TEST ===\n",
            "\n",
            "Q1: What is Federated Learning?\n",
            "Answer: A distributed machine learning approach where multiple devices collaboratively train a global model while keeping their local data private.\n",
            "\n",
            "Q2: What is Fed-Meta-Align?\n",
            "Answer: A federated TinyML framework that aligns model updates across heterogeneous devices using similarity-aware aggregation and personalization.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}